{"ID":"20230817161804-8icdfva","Spec":"1","Type":"NodeDocument","Properties":{"id":"20230817161804-8icdfva","title":"性能优化方案","updated":"20230817161804"},"Children":[{"ID":"20230817161805-4nprpyx","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20230817161805-4nprpyx","updated":"20230817161805"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"## ","Properties":{"id":""}},{"Type":"NodeText","Data":"性能优化方案"}]},{"ID":"20230817161806-wcauxi7","Type":"NodeParagraph","Properties":{"id":"20230817161806-wcauxi7","updated":"20230817161806"},"Children":[{"Type":"NodeText","Data":"下面我们通过这几个方式来实现对Spark程序的性能优化"}]},{"ID":"20230817161807-voqqu2f","Type":"NodeList","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161807-voqqu2f","updated":"20230817161807"},"Children":[{"ID":"20230817161808-y1xrlka","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161808-y1xrlka","updated":"20230817161808"},"Children":[{"ID":"20230817161809-d95if2q","Type":"NodeParagraph","Properties":{"id":"20230817161809-d95if2q","updated":"20230817161809"},"Children":[{"Type":"NodeText","Data":"高性能序列化类库"}]}]},{"ID":"20230817161810-ew2a9j7","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161810-ew2a9j7","updated":"20230817161810"},"Children":[{"ID":"20230817161811-tvxuvjb","Type":"NodeParagraph","Properties":{"id":"20230817161811-tvxuvjb","updated":"20230817161811"},"Children":[{"Type":"NodeText","Data":"持久化或者checkpoint"}]}]},{"ID":"20230817161812-ckob6i6","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161812-ckob6i6","updated":"20230817161812"},"Children":[{"ID":"20230817161813-2o9w53i","Type":"NodeParagraph","Properties":{"id":"20230817161813-2o9w53i","updated":"20230817161813"},"Children":[{"Type":"NodeText","Data":"JVM垃圾回收调优"}]}]},{"ID":"20230817161814-1f7saue","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161814-1f7saue","updated":"20230817161814"},"Children":[{"ID":"20230817161815-7vfj4pj","Type":"NodeParagraph","Properties":{"id":"20230817161815-7vfj4pj","updated":"20230817161815"},"Children":[{"Type":"NodeText","Data":"提高并行度"}]}]},{"ID":"20230817161816-qdleg4j","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161816-qdleg4j","updated":"20230817161816"},"Children":[{"ID":"20230817161817-jwb35wv","Type":"NodeParagraph","Properties":{"id":"20230817161817-jwb35wv","updated":"20230817161817"},"Children":[{"Type":"NodeText","Data":"数据本地化"}]}]},{"ID":"20230817161818-odctvy2","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161818-odctvy2","updated":"20230817161818"},"Children":[{"ID":"20230817161819-8d6leuj","Type":"NodeParagraph","Properties":{"id":"20230817161819-8d6leuj","updated":"20230817161819"},"Children":[{"Type":"NodeText","Data":"算子优化"}]}]}]},{"ID":"20230817161820-rmu972b","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20230817161820-rmu972b","updated":"20230817161820"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"### ","Properties":{"id":""}},{"Type":"NodeText","Data":"高性能序列化类库"}]},{"ID":"20230817161821-bgy27wq","Type":"NodeParagraph","Properties":{"id":"20230817161821-bgy27wq","updated":"20230817161821"},"Children":[{"Type":"NodeText","Data":"在任何分布式系统中，序列化都是扮演着一个重要的角色的。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"如果使用的序列化技术，在执行序列化操作的时候很慢，或者是序列化后的数据还是很大，那么会让分布式应用程序的性能下降很多。所以，进行Spark性能优化的第一步，就是进行序列化的性能优化。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"Spark默认会在一些地方对数据进行序列化，如果我们的算子函数使用到了外部的数据（比如Java中的自定义类型），那么也需要让其可序列化，否则程序在执行的时候是会报错的，提示没有实现序列化，这个一定要注意。"}]},{"ID":"20230817161822-vg35dzs","Type":"NodeParagraph","Properties":{"id":"20230817161822-vg35dzs","updated":"20230817161822"},"Children":[{"Type":"NodeText","Data":"原因是这样的："},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"因为Spark的初始化工作是在Driver进程中进行的，但是实际执行是在Worker节点的Executor进程中进行的；当Executor端需要用到Driver端封装的对象时，就需要把Driver端的对象通过序列化传输到Executor端，这个对象就需要实现序列化。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"否则会报错，提示对象没有实现序列化"}]},{"ID":"20230817161823-r2szooe","Type":"NodeBlockquote","Properties":{"id":"20230817161823-r2szooe","updated":"20230817161823"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e ","Properties":{"id":""}},{"ID":"20230817161824-firdjrc","Type":"NodeParagraph","Properties":{"id":"20230817161824-firdjrc","updated":"20230817161824"},"Children":[{"Type":"NodeText","Data":"注意了，其实遇到这种没有实现序列化的对象，解决方法有两种"}]}]},{"ID":"20230817161825-3x3edo3","Type":"NodeList","ListData":{"Typ":1,"Tight":true,"Start":1,"Delimiter":46,"Padding":3,"Marker":"MQ==","Num":1},"Properties":{"id":"20230817161825-3x3edo3","updated":"20230817161825"},"Children":[{"ID":"20230817161826-7rz1x4n","Type":"NodeListItem","Data":"1","ListData":{"Typ":1,"Tight":true,"Start":1,"Delimiter":46,"Padding":3,"Marker":"MQ==","Num":1},"Properties":{"id":"20230817161826-7rz1x4n","updated":"20230817161826"},"Children":[{"ID":"20230817161827-bv6abpy","Type":"NodeParagraph","Properties":{"id":"20230817161827-bv6abpy","updated":"20230817161827"},"Children":[{"Type":"NodeText","Data":"如果此对象可以支持序列化，则将其实现Serializable接口，让它支持序列化"}]}]},{"ID":"20230817161828-bj9utt1","Type":"NodeListItem","Data":"2","ListData":{"Typ":1,"Tight":true,"Start":2,"Delimiter":46,"Padding":3,"Marker":"Mg==","Num":2},"Properties":{"id":"20230817161828-bj9utt1","updated":"20230817161828"},"Children":[{"ID":"20230817161829-agzinsv","Type":"NodeParagraph","Properties":{"id":"20230817161829-agzinsv","updated":"20230817161829"},"Children":[{"Type":"NodeText","Data":"如果此对象不支持序列化，针对一些数据库连接之类的对象，这种对象是不支持序列化的，所以可以把这个代码放到算子内部，这样就不会通过driver端传过去了，它会直接在executor中执行。"}]}]}]},{"ID":"20230817161830-09wq4rz","Type":"NodeParagraph","Properties":{"id":"20230817161830-09wq4rz","updated":"20230817161830"},"Children":[{"Type":"NodeText","Data":"Spark对于序列化的便捷性和性能进行了一个取舍和权衡。默认情况下，Spark倾向于序列化的便捷性，使用了Java自身提供的序列化机制——基于"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"ObjectInputStream"},{"Type":"NodeText","Data":"和"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"ObjectOutputStream"},{"Type":"NodeText","Data":"的序列化机制，因为这种方式是Java原生提供的，使用起来比较方便，"}]},{"ID":"20230817161831-rj456qz","Type":"NodeParagraph","Properties":{"id":"20230817161831-rj456qz","updated":"20230817161831"},"Children":[{"Type":"NodeText","Data":"但是Java序列化机制的性能并不高。序列化的速度相对较慢，而且序列化以后的数据，相对来说还是比较大，比较占空间。所以，如果你的Spark应用程序对内存很敏感，那默认的Java序列化机制并不是最好的选择。"}]},{"ID":"20230817161832-qhnv5gj","Type":"NodeParagraph","Properties":{"id":"20230817161832-qhnv5gj","updated":"20230817161832"},"Children":[{"Type":"NodeText","Data":"Spark实际上提供了两种序列化机制："},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"Java序列化机制和Kryo序列化机制"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"Spark只是默认使用了java这种序列化机制"}]},{"ID":"20230817161833-107snrq","Type":"NodeList","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161833-107snrq","updated":"20230817161833"},"Children":[{"ID":"20230817161834-3mj5xfk","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161834-3mj5xfk","updated":"20230817161834"},"Children":[{"ID":"20230817161835-kx7nug5","Type":"NodeParagraph","Properties":{"id":"20230817161835-kx7nug5","updated":"20230817161835"},"Children":[{"Type":"NodeText","Data":"Java序列化机制：默认情况下，Spark使用Java自身的ObjectInputStream和ObjectOutputStream机制进行对象的序列化。只要你的类实现了Serializable接口，那么都是可以序列化的。Java序列化机制的速度比较慢，而且序列化后的数据占用的内存空间比较大，这是它的缺点"}]}]},{"ID":"20230817161836-doqzocg","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161836-doqzocg","updated":"20230817161836"},"Children":[{"ID":"20230817161837-efjk69f","Type":"NodeParagraph","Properties":{"id":"20230817161837-efjk69f","updated":"20230817161837"},"Children":[{"Type":"NodeText","Data":"Kryo序列化机制：Spark也支持使用Kryo序列化。Kryo序列化机制比Java序列化机制更快，而且序列化后的数据占用的空间更小，通常比Java序列化的数据占用的空间要小10倍左右。"}]}]}]},{"ID":"20230817161838-v124fgs","Type":"NodeBlockquote","Properties":{"id":"20230817161838-v124fgs","updated":"20230817161838"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e ","Properties":{"id":""}},{"ID":"20230817161839-ineivkn","Type":"NodeParagraph","Properties":{"id":"20230817161839-ineivkn","updated":"20230817161839"},"Children":[{"Type":"NodeText","Data":"Kryo序列化机制之所以不是默认序列化机制的原因："}]}]},{"ID":"20230817161840-bgrjrvl","Type":"NodeParagraph","Properties":{"id":"20230817161840-bgrjrvl","updated":"20230817161840"},"Children":[{"Type":"NodeText","Data":"第一点：因为有些类型虽然实现了Seriralizable接口，但是它也不一定能够被Kryo进行序列化；"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"第二点：如果你要得到最佳的性能，Kryo还要求你在Spark应用程序中，对所有你需要序列化的类型都进行手工注册，这样就比较麻烦了"}]},{"ID":"20230817161841-7j4euwz","Type":"NodeParagraph","Properties":{"id":"20230817161841-7j4euwz","updated":"20230817161841"},"Children":[{"Type":"NodeText","Data":"如果要使用Kryo序列化机制"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"首先要用"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"SparkConf"},{"Type":"NodeText","Data":"设置"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"spark.serializer"},{"Type":"NodeText","Data":"的值为"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"org.apache.spark.serializer.KryoSerializer"},{"Type":"NodeText","Data":"，就是将Spark的序列化器设置为"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"KryoSerializer"},{"Type":"NodeText","Data":"。这样，Spark在进行序列化时，就会使用Kryo进行序列化了。"}]},{"ID":"20230817161842-an7fzt4","Type":"NodeParagraph","Properties":{"id":"20230817161842-an7fzt4","updated":"20230817161842"},"Children":[{"Type":"NodeText","Data":"使用Kryo时针对需要序列化的类，需要预先进行注册，这样才能获得最佳性能——如果不注册的话，Kryo也能正常工作，只是Kryo必须时刻保存类型的全类名，反而占用不少内存。"}]},{"ID":"20230817161843-ds1144b","Type":"NodeParagraph","Properties":{"id":"20230817161843-ds1144b","updated":"20230817161843"},"Children":[{"Type":"NodeText","Data":"Spark默认对Scala中常用的类型在Kryo中做了注册，但是，如果在自己的算子中，使用了外部的自定义类型的对象，那么还是需要对其进行注册。"}]},{"ID":"20230817161844-rozgp6a","Type":"NodeParagraph","Properties":{"id":"20230817161844-rozgp6a","updated":"20230817161844"},"Children":[{"Type":"NodeText","Data":"注册自定义的数据类型格式："}]},{"ID":"20230817161845-1lwxabi","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161845-1lwxabi","updated":"20230817161845"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"conf.registerKryoClasses(...)\n代码块1\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161846-524ood2","Type":"NodeBlockquote","Properties":{"id":"20230817161846-524ood2","updated":"20230817161846"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e ","Properties":{"id":""}},{"ID":"20230817161847-mm669uz","Type":"NodeParagraph","Properties":{"id":"20230817161847-mm669uz","updated":"20230817161847"},"Children":[{"Type":"NodeText","Data":"注意：如果要序列化的自定义的类型，字段特别多，此时就需要对Kryo本身进行优化，因为Kryo内部的缓存可能不够存放那么大的class对象"}]}]},{"ID":"20230817161848-g5qtmw2","Type":"NodeParagraph","Properties":{"id":"20230817161848-g5qtmw2","updated":"20230817161848"},"Children":[{"Type":"NodeText","Data":"需要调用"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"SparkConf.set()"},{"Type":"NodeText","Data":"方法，设置"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"spark.kryoserializer.buffer.mb"},{"Type":"NodeText","Data":"参数的值，将其调大，默认值为"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"2"},{"Type":"NodeText","Data":"，单位是"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"MB"},{"Type":"NodeText","Data":"，也就是说最大能缓存"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"2M"},{"Type":"NodeText","Data":"的对象，然后进行序列化。可以在必要时将其调大。"}]},{"ID":"20230817161849-6ry95g1","Type":"NodeBlockquote","Properties":{"id":"20230817161849-6ry95g1","updated":"20230817161849"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e ","Properties":{"id":""}},{"ID":"20230817161850-olfyw92","Type":"NodeParagraph","Properties":{"id":"20230817161850-olfyw92","updated":"20230817161850"},"Children":[{"Type":"NodeText","Data":"什么场景下适合使用Kryo序列化？"}]}]},{"ID":"20230817161851-averg85","Type":"NodeParagraph","Properties":{"id":"20230817161851-averg85","updated":"20230817161851"},"Children":[{"Type":"NodeText","Data":"一般是针对一些自定义的对象，例如我们自己定义了一个对象，这个对象里面包含了几十M，或者上百M的数据，然后在算子函数内部，使用到了这个外部的大对象"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"如果默认情况下，让Spark用java序列化机制来序列化这种外部的大对象，那么就会导致序列化速度比较慢，并且序列化以后的数据还是比较大。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"所以，在这种情况下，比较适合使用Kryo序列化类库，来对外部的大对象进行序列化，提高序列化速度，减少序列化后的内存空间占用。"}]},{"ID":"20230817161852-p2gvblm","Type":"NodeParagraph","Properties":{"id":"20230817161852-p2gvblm","updated":"20230817161852"},"Children":[{"Type":"NodeText","Data":"用代码实现一个案例："},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"scala代码如下："}]},{"ID":"20230817161853-o84exjc","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161853-o84exjc","updated":"20230817161853"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"package com.imooc.scala\n\nimport com.esotericsoftware.kryo.Kryo\nimport org.apache.spark.serializer.KryoRegistrator\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.{SparkConf, SparkContext}\n\n/**\n * 需求：Kryo序列化的使用\n * Created by xuwei\n */\nobject KryoSerScala {\n\n  def main(args: Array[String]): Unit = {\n    val conf = new SparkConf()\n    conf.setAppName(\"KryoSerScala\")\n      .setMaster(\"local\")\n      //指定使用kryo序列化机制，注意：如果使用了registerKryoClasses，其实这一行设置是可以省略的\n      .set(\"spark.serializer\",\"org.apache.spark.serializer.KryoSerializer\")\n      .registerKryoClasses(Array(classOf[Person]))//注册自定义的数据类型\n    val sc = new SparkContext(conf)\n\n    val dataRDD = sc.parallelize(Array(\"hello you\",\"hello me\"))\n    val wordsRDD = dataRDD.flatMap(_.split(\" \"))\n    val personRDD = wordsRDD.map(word=\u003ePerson(word,18)).persist(StorageLevel.MEMORY_ONLY_SER)\n    personRDD.foreach(println(_))\n\n      //while循环是为了保证程序不结束，方便在本地查看4040页面中的storage信息\n    while (true) {\n      ;\n    }\n  }\n\n}\ncase class Person(name: String,age: Int) extends Serializable\n代码块1234567891011121314151617181920212223242526272829303132333435\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161854-sn3ruhw","Type":"NodeParagraph","Properties":{"id":"20230817161854-sn3ruhw","updated":"20230817161854"},"Children":[{"Type":"NodeText","Data":"执行任务，然后访问localhost的4040界面"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"在界面中可以看到cache的数据大小是"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"31"},{"Type":"NodeText","Data":"字节。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeImage","Properties":{"id":""},"Children":[{"Type":"NodeBang","Data":"!","Properties":{"id":""}},{"Type":"NodeOpenBracket","Data":"[","Properties":{"id":""}},{"Type":"NodeLinkText","Data":"图片描述","Properties":{"id":""}},{"Type":"NodeCloseBracket","Data":"]","Properties":{"id":""}},{"Type":"NodeOpenParen","Data":"(","Properties":{"id":""}},{"Type":"NodeLinkDest","Data":"https://img.mukewang.com/wiki/5f5649eb09027c1112650341.jpg","Properties":{"id":""}},{"Type":"NodeCloseParen","Data":")","Properties":{"id":""}}]}]},{"ID":"20230817161855-8yddpdt","Type":"NodeParagraph","Properties":{"id":"20230817161855-8yddpdt","updated":"20230817161855"},"Children":[{"Type":"NodeText","Data":"那我们把kryo序列化设置去掉，使用默认的java序列化看一下效果"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"修改代码，注释掉这两行代码即可"}]},{"ID":"20230817161856-uez9i0r","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161856-uez9i0r","updated":"20230817161856"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"//.set(\"spark.serializer\",\"org.apache.spark.serializer.KryoSerializer\")\n//.registerKryoClasses(Array(classOf[Person]))\n代码块12\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161857-cdahtjt","Type":"NodeParagraph","Properties":{"id":"20230817161857-cdahtjt","updated":"20230817161857"},"Children":[{"Type":"NodeText","Data":"运行任务，再访问4040界面"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"发现此时占用的内存空间是"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"138"},{"Type":"NodeText","Data":"字节，比使用kryo的方式内存空间多占用了将近5倍。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"所以从这可以看出来，使用"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"kryo"},{"Type":"NodeText","Data":"序列化方式对内存的占用会降低很多。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeImage","Properties":{"id":""},"Children":[{"Type":"NodeBang","Data":"!","Properties":{"id":""}},{"Type":"NodeOpenBracket","Data":"[","Properties":{"id":""}},{"Type":"NodeLinkText","Data":"图片描述","Properties":{"id":""}},{"Type":"NodeCloseBracket","Data":"]","Properties":{"id":""}},{"Type":"NodeOpenParen","Data":"(","Properties":{"id":""}},{"Type":"NodeLinkDest","Data":"https://img.mukewang.com/wiki/5f564a1b09a53a5412670318.jpg","Properties":{"id":""}},{"Type":"NodeCloseParen","Data":")","Properties":{"id":""}}]}]},{"ID":"20230817161858-ztuqlnu","Type":"NodeBlockquote","Properties":{"id":"20230817161858-ztuqlnu","updated":"20230817161858"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e ","Properties":{"id":""}},{"ID":"20230817161859-xy84lvx","Type":"NodeParagraph","Properties":{"id":"20230817161859-xy84lvx","updated":"20230817161859"},"Children":[{"Type":"NodeText","Data":"注意：如果我们只是将spark的序列化机制改为了kryo序列化，但是没有对使用到的自定义类型手工进行注册，那么此时内存的占用会介于前面两种情况之间"}]}]},{"ID":"20230817161860-32gn5o7","Type":"NodeParagraph","Properties":{"id":"20230817161860-32gn5o7","updated":"20230817161860"},"Children":[{"Type":"NodeText","Data":"修改代码，只注释掉registerKryoClasses这一行代码"}]},{"ID":"20230817161861-14cvgcz","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161861-14cvgcz","updated":"20230817161861"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":".set(\"spark.serializer\",\"org.apache.spark.serializer.KryoSerializer\")\n//.registerKryoClasses(Array(classOf[Person]))//注册自定义的数据类型\n代码块12\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161862-iex6pbk","Type":"NodeParagraph","Properties":{"id":"20230817161862-iex6pbk","updated":"20230817161862"},"Children":[{"Type":"NodeText","Data":"运行任务，再访问4040界面"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"发现此时的内存占用为"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"123"},{"Type":"NodeText","Data":"字节，介于前面的"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"31"},{"Type":"NodeText","Data":"字节和"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"138"},{"Type":"NodeText","Data":"字节之间。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"所以从这可以看出来，在使用kryo序列化的时候，针对自定义的类型最好是手工注册一下，否则就算开启了kryo序列化，性能的提升也是有限的。"}]},{"ID":"20230817161863-pzq534e","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20230817161863-pzq534e","updated":"20230817161863"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"### ","Properties":{"id":""}},{"Type":"NodeText","Data":"持久化或者checkpoint"}]},{"ID":"20230817161864-s0v0qpr","Type":"NodeParagraph","Properties":{"id":"20230817161864-s0v0qpr","updated":"20230817161864"},"Children":[{"Type":"NodeText","Data":"针对程序中多次被transformation或者action操作的RDD进行持久化操作，避免对一个RDD反复进行计算，再进一步优化，使用Kryo序列化的持久化级别，减少内存占用"}]},{"ID":"20230817161865-v7fxmvp","Type":"NodeParagraph","Properties":{"id":"20230817161865-v7fxmvp","updated":"20230817161865"},"Children":[{"Type":"NodeText","Data":"为了保证RDD持久化数据在可能丢失的情况下还能实现高可靠，则需要对RDD执行Checkpoint操作"}]},{"ID":"20230817161866-kl7p8ln","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20230817161866-kl7p8ln","updated":"20230817161866"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"### ","Properties":{"id":""}},{"Type":"NodeText","Data":"JVM垃圾回收调优"}]},{"ID":"20230817161867-04gmmn8","Type":"NodeParagraph","Properties":{"id":"20230817161867-04gmmn8","updated":"20230817161867"},"Children":[{"Type":"NodeText","Data":"由于Spark是基于内存的计算引擎，RDD缓存的数据，以及算子执行期间创建的对象都是放在内存中的，所以针对Spark任务如果内存设置不合理会导致大部分时间都消耗在垃圾回收上"}]},{"ID":"20230817161868-v8wqmuq","Type":"NodeParagraph","Properties":{"id":"20230817161868-v8wqmuq","updated":"20230817161868"},"Children":[{"Type":"NodeText","Data":"对于垃圾回收来说，最重要的就是调节RDD缓存占用的内存空间，和算子执行时创建的对象占用的内存空间的比例。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"默认情况下，Spark使用每个"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"executor 60%"},{"Type":"NodeText","Data":"的内存空间来缓存RDD，那么只有"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"40%"},{"Type":"NodeText","Data":"的内存空间来存放算子执行期间创建的对象"}]},{"ID":"20230817161869-6js2raq","Type":"NodeParagraph","Properties":{"id":"20230817161869-6js2raq","updated":"20230817161869"},"Children":[{"Type":"NodeText","Data":"在这种情况下，可能由于内存空间的不足，并且算子对应的task任务在运行时创建的对象过大，那么一旦发现"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"40%"},{"Type":"NodeText","Data":"的内存空间不够用了，就会触发Java虚拟机的垃圾回收操作。因此在极端情况下，垃圾回收操作可能会被频繁触发。"}]},{"ID":"20230817161870-isk3op4","Type":"NodeParagraph","Properties":{"id":"20230817161870-isk3op4","updated":"20230817161870"},"Children":[{"Type":"NodeText","Data":"在这种情况下，如果发现垃圾回收频繁发生。那么就需要对这个比例进行调优了，"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"spark.storage.memoryFraction"},{"Type":"NodeText","Data":"参数的值默认是"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"0.6"},{"Type":"NodeText","Data":"。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"使用"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"SparkConf().set(\u0026quot;spark.storage.memoryFraction\u0026quot;, \u0026quot;0.5\u0026quot;)"},{"Type":"NodeText","Data":"可以进行修改，就是将RDD缓存占用内存空间的比例降低为"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"50%"},{"Type":"NodeText","Data":"，从而提供更多的内存空间来保存task运行时创建的对象。"}]},{"ID":"20230817161871-xt6w6d3","Type":"NodeParagraph","Properties":{"id":"20230817161871-xt6w6d3","updated":"20230817161871"},"Children":[{"Type":"NodeText","Data":"因此，对于RDD持久化而言，完全可以使用Kryo序列化，加上降低其executor内存占比的方式，来减少其内存消耗。给task提供更多的内存，从而避免task在执行时频繁触发垃圾回收。"}]},{"ID":"20230817161872-i8cygs4","Type":"NodeParagraph","Properties":{"id":"20230817161872-i8cygs4","updated":"20230817161872"},"Children":[{"Type":"NodeText","Data":"我们可以对task的垃圾回收进行监测，在spark的任务执行界面，可以查看每个task执行消耗的时间，以及task gc消耗的时间。"}]},{"ID":"20230817161873-4xq43u0","Type":"NodeHeading","HeadingLevel":4,"Properties":{"id":"20230817161873-4xq43u0","updated":"20230817161873"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"#### ","Properties":{"id":""}},{"Type":"NodeText","Data":"GC"}]},{"ID":"20230817161874-f5ur37g","Type":"NodeParagraph","Properties":{"id":"20230817161874-f5ur37g","updated":"20230817161874"},"Children":[{"Type":"NodeText","Data":"既然说到了Java中的GC，那我们就需要说道说道了。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"Java堆空间被划分成了两块空间：一个是年轻代，一个是老年代。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"年轻代放的是短时间存活的对象"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"老年代放的是长时间存活的对象。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"年轻代又被划分了三块空间，"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"Eden、Survivor1、Survivor2"}]},{"ID":"20230817161875-dpent9p","Type":"NodeParagraph","Properties":{"id":"20230817161875-dpent9p","updated":"20230817161875"},"Children":[{"Type":"NodeText","Data":"来看一下这个内存划分比例图"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeImage","Properties":{"id":""},"Children":[{"Type":"NodeBang","Data":"!","Properties":{"id":""}},{"Type":"NodeOpenBracket","Data":"[","Properties":{"id":""}},{"Type":"NodeLinkText","Data":"图片描述","Properties":{"id":""}},{"Type":"NodeCloseBracket","Data":"]","Properties":{"id":""}},{"Type":"NodeOpenParen","Data":"(","Properties":{"id":""}},{"Type":"NodeLinkDest","Data":"https://img.mukewang.com/wiki/5f564abf0999298317370398.jpg","Properties":{"id":""}},{"Type":"NodeCloseParen","Data":")","Properties":{"id":""}}]}]},{"ID":"20230817161876-y0no54y","Type":"NodeBlockquote","Properties":{"id":"20230817161876-y0no54y","updated":"20230817161876"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e ","Properties":{"id":""}},{"ID":"20230817161877-mqk7owq","Type":"NodeParagraph","Properties":{"id":"20230817161877-mqk7owq","updated":"20230817161877"},"Children":[{"Type":"NodeText","Data":"年轻代占堆内存的1/3，老年代占堆内存的2/3"}]}]},{"ID":"20230817161878-d6gt3av","Type":"NodeParagraph","Properties":{"id":"20230817161878-d6gt3av","updated":"20230817161878"},"Children":[{"Type":"NodeText","Data":"其中年轻代又被划分了三块，"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"Eden，Survivor1，Survivor2"},{"Type":"NodeText","Data":"的比例为"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"8:1:1"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"Eden区域和Survivor1区域用于存放对象，Survivor2区域备用。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"我们创建的对象，首先会放入Eden区域，如果Eden区域满了，那么就会触发一次Minor GC，进行年轻代的垃圾回收(其实就是回收Eden区域内没有人使用的对象)，然后将存活的对象存入Survivor1区域，再创建对象的时候继续放入Eden区域。第二次Eden区域满了，那么Eden和Survivor1区域中存活的对象，会一块被移动到Survivor2区域中。然后Survivor1和Survivor2的角色调换，Survivor1变成了备用。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"当第三次Eden区域再满了的时候，Eden和Survivor2区域中存活的对象，会一块被移动到Survivor1区域中，按照这个规律进行循环"}]},{"ID":"20230817161879-0ci8z7r","Type":"NodeParagraph","Properties":{"id":"20230817161879-0ci8z7r","updated":"20230817161879"},"Children":[{"Type":"NodeText","Data":"如果一个对象，在年轻代中，撑过了多次垃圾回收(默认是15次)，都没有被回收掉，那么会被认为是长时间存活的，此时就会被移入老年代。此外，如果在将Eden和Survivor1中的存活对象，尝试放入Survivor2中时，发现Survivor2放满了，那么会直接放入老年代。此时就出现了，短时间存活的对象，也会进入老年代的问题。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"如果老年代的空间满了，那么就会触发Full GC，进行老年代的垃圾回收操作，如果执行Full GC也释放不了内存空间，就会报内存溢出的错误了。"}]},{"ID":"20230817161880-xopvrjj","Type":"NodeBlockquote","Properties":{"id":"20230817161880-xopvrjj","updated":"20230817161880"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e ","Properties":{"id":""}},{"ID":"20230817161881-c3c04tt","Type":"NodeParagraph","Properties":{"id":"20230817161881-c3c04tt","updated":"20230817161881"},"Children":[{"Type":"NodeText","Data":"注意了，Full GC是一个重量级的垃圾回收，Full GC执行的时候，程序是处于暂停状态的，这样会非常影响性能。"}]}]},{"ID":"20230817161882-m5xqyo6","Type":"NodeParagraph","Properties":{"id":"20230817161882-m5xqyo6","updated":"20230817161882"},"Children":[{"Type":"NodeText","Data":"Spark中，垃圾回收调优的目标就是，只有真正长时间存活的对象，才能进入老年代，短时间存活的对象，只能呆在年轻代。不能因为某个Survivor区域空间不够，在Minor GC时，就进入了老年代，从而造成短时间存活的对象，长期呆在老年代中占据了空间，这样Full GC时要回收大量的短时间存活的对象，导致Full GC速度缓慢。"}]},{"ID":"20230817161883-4q2tdsl","Type":"NodeParagraph","Properties":{"id":"20230817161883-4q2tdsl","updated":"20230817161883"},"Children":[{"Type":"NodeText","Data":"如果发现，在task执行期间，大量full gc发生了，那么说明，年轻代的Eden区域，给的空间不够大。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"此时可以执行一些操作来优化垃圾回收行为"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"1：最直接的就是提高Executor的内存"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"在spark-submit中通过参数指定executor的内存"}]},{"ID":"20230817161884-jsopksl","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161884-jsopksl","updated":"20230817161884"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"--executor-memory 1G \n代码块1\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161885-dao3wmd","Type":"NodeParagraph","Properties":{"id":"20230817161885-dao3wmd","updated":"20230817161885"},"Children":[{"Type":"NodeText","Data":"2：调整Eden与s1和s2的比值【一般情况下不建议调整这块的比值】"}]},{"ID":"20230817161886-t2antmw","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20230817161886-t2antmw","updated":"20230817161886"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"### ","Properties":{"id":""}},{"Type":"NodeText","Data":"提高并行度"}]},{"ID":"20230817161887-jfh4eqt","Type":"NodeParagraph","Properties":{"id":"20230817161887-jfh4eqt","updated":"20230817161887"},"Children":[{"Type":"NodeText","Data":"实际上Spark集群的资源并不一定会被充分利用到，所以要尽量设置合理的并行度，来充分地利用集群的资源，这样才能提高Spark程序的性能。"}]},{"ID":"20230817161888-6q0ij15","Type":"NodeParagraph","Properties":{"id":"20230817161888-6q0ij15","updated":"20230817161888"},"Children":[{"Type":"NodeText","Data":"Spark会自动设置以文件作为输入源的RDD的并行度，依据其大小，比如HDFS，就会给每一个block创建一个partition，也依据这个设置并行度。对于reduceByKey等会发生shuffle操作的算子，会使用并行度最大的父RDD的并行度"}]},{"ID":"20230817161889-5wjs0ka","Type":"NodeParagraph","Properties":{"id":"20230817161889-5wjs0ka","updated":"20230817161889"},"Children":[{"Type":"NodeText","Data":"可以手动使用"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"textFile()、parallelize()"},{"Type":"NodeText","Data":"等方法的第二个参数来设置并行度；也可以使用"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"spark.default.parallelism"},{"Type":"NodeText","Data":"参数，来设置统一的并行度。Spark官方的推荐是，给集群中的每个cpu core设置"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"2~3"},{"Type":"NodeText","Data":"个task。"}]},{"ID":"20230817161890-g3n7u6u","Type":"NodeParagraph","Properties":{"id":"20230817161890-g3n7u6u","updated":"20230817161890"},"Children":[{"Type":"NodeText","Data":"下面来举个例子"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"我在"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"spark-submit"},{"Type":"NodeText","Data":"脚本中给任务设置了5 个executor，每个executor，设置了2个cpu core"}]},{"ID":"20230817161891-r61gcvz","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161891-r61gcvz","updated":"20230817161891"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"spark-submit \\\n--master yarn \\\n--deploy-mode cluster \\\n--executor-memory 1G \\\n--num-executors 5 \\\n--executor-cores 2 \\\n.....\n代码块1234567\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161892-3qkj15v","Type":"NodeParagraph","Properties":{"id":"20230817161892-3qkj15v","updated":"20230817161892"},"Children":[{"Type":"NodeText","Data":"此时，如果我在代码中设置了默认并行度为5"}]},{"ID":"20230817161893-9n6fsjr","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161893-9n6fsjr","updated":"20230817161893"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"conf.set(\"spark.default.parallelism\",\"5\")\n代码块1\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161894-5ve6uh0","Type":"NodeParagraph","Properties":{"id":"20230817161894-5ve6uh0","updated":"20230817161894"},"Children":[{"Type":"NodeText","Data":"这个参数设置完了以后，也就意味着所有RDD的partition都被设置成了5个，针对RDD的每一个partition，spark会启动一个task来进行计算，所以对于所有的算子操作，都只会创建5个task来处理对应的RDD中的数据。"}]},{"ID":"20230817161895-xvaji46","Type":"NodeBlockquote","Properties":{"id":"20230817161895-xvaji46","updated":"20230817161895"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e ","Properties":{"id":""}},{"ID":"20230817161896-o2faq4g","Type":"NodeParagraph","Properties":{"id":"20230817161896-o2faq4g","updated":"20230817161896"},"Children":[{"Type":"NodeText","Data":"但是注意了，我们前面在spark-submit脚本中设置了5个executor，每个executor 2个cpu core，所以这个时候spark其实会向yarn集群申请10个cpu core，但是我们在代码中设置了默认并行度为5，只会产生5个task，一个task使用一个cpu core，那也就意味着有5个cpu core是空闲的，这样申请的资源就浪费了一半。"}]}]},{"ID":"20230817161897-rutzy6w","Type":"NodeParagraph","Properties":{"id":"20230817161897-rutzy6w","updated":"20230817161897"},"Children":[{"Type":"NodeText","Data":"其实最好的情况，就是每个cpu core都不闲着，一直在运行，这样可以达到资源的最大使用率，其实让一个cpu core运行一个task都是有点浪费的，官方也建议让每个cpu core运行2~3个task，这样可以充分压榨CPU的性能"}]},{"ID":"20230817161898-kiue2en","Type":"NodeBlockquote","Properties":{"id":"20230817161898-kiue2en","updated":"20230817161898"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e ","Properties":{"id":""}},{"ID":"20230817161899-hjtw1wl","Type":"NodeParagraph","Properties":{"id":"20230817161899-hjtw1wl","updated":"20230817161899"},"Children":[{"Type":"NodeText","Data":"为什么这样说呢？"}]}]},{"ID":"20230817161900-gr7o4k4","Type":"NodeParagraph","Properties":{"id":"20230817161900-gr7o4k4","updated":"20230817161900"},"Children":[{"Type":"NodeText","Data":"是这样的，因为每个task执行的顺序和执行结束的时间很大概率是不一样的，如果正好有10个cpu，运行10个taks，那么某个task可能很快就执行完了，那么这个CPU就空闲下来了，这样资源就浪费了。"}]},{"ID":"20230817161901-wcxr9p6","Type":"NodeParagraph","Properties":{"id":"20230817161901-wcxr9p6","updated":"20230817161901"},"Children":[{"Type":"NodeText","Data":"所以说官方推荐，给每个cpu分配2~3个task是比较合理的，可以充分利用CPU资源，发挥它最大的价值。"}]},{"ID":"20230817161902-b9l0c1h","Type":"NodeParagraph","Properties":{"id":"20230817161902-b9l0c1h","updated":"20230817161902"},"Children":[{"Type":"NodeText","Data":"下面我们来实际写个案例看一下效果"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"Scala代码如下："}]},{"ID":"20230817161903-44hafyu","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161903-44hafyu","updated":"20230817161903"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"package com.imooc.scala\n\nimport org.apache.spark.{SparkConf, SparkContext}\n\n/**\n * 需求：设置并行度\n * 1：可以在textFile或者parallelize等方法的第二个参数中设置并行度\n * 2：或者通过spark.default.parallelism参数统一设置并行度\n * Created by xuwei\n */\nobject MoreParallelismScala {\n\n  def main(args: Array[String]): Unit = {\n    val conf = new SparkConf()\n    conf.setAppName(\"MoreParallelismScala\")\n\n    //设置全局并行度\n    conf.set(\"spark.default.parallelism\",\"5\")\n\n    val sc = new SparkContext(conf)\n\n    val dataRDD = sc.parallelize(Array(\"hello\",\"you\",\"hello\",\"me\",\"hehe\",\"hello\",\"you\",\"hello\",\"me\",\"hehe\"))\n    dataRDD.map((_,1))\n      .reduceByKey(_ + _)\n      .foreach(println(_))\n\n    sc.stop()\n  }\n\n}\n代码块预览复制123456789101112131415161718192021222324252627282930\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161904-c1r0ops","Type":"NodeParagraph","Properties":{"id":"20230817161904-c1r0ops","updated":"20230817161904"},"Children":[{"Type":"NodeText","Data":"如果想要最大限度利用CPU的性能，至少将"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"spark.default.parallelism"},{"Type":"NodeText","Data":"的值设置为10，这样可以实现一个cpu运行一个task，其实官方推荐是设置为20或者30。"}]},{"ID":"20230817161905-mksdgzq","Type":"NodeParagraph","Properties":{"id":"20230817161905-mksdgzq","updated":"20230817161905"},"Children":[{"Type":"NodeText","Data":"其实这个参数也可以在spark-submit脚本中动态设置，通过"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"--conf"},{"Type":"NodeText","Data":"参数设置，这样就比较灵活了。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"注意：此时需要将代码中设置"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"spark.default.parallelism"},{"Type":"NodeText","Data":"的配置注释掉"}]},{"ID":"20230817161906-wg06w40","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161906-wg06w40","updated":"20230817161906"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"//conf.set(\"spark.default.parallelism\",\"5\")\n代码块1\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161907-v7ai3ww","Type":"NodeParagraph","Properties":{"id":"20230817161907-v7ai3ww","updated":"20230817161907"},"Children":[{"Type":"NodeText","Data":"再封装一个脚本"}]},{"ID":"20230817161908-ab6p7bx","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161908-ab6p7bx","updated":"20230817161908"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"[root@bigdata04 sparkjars]# vi moreParallelismJob2.sh \nspark-submit \\\n--class com.imooc.scala.MoreParallelismScala \\\n--master yarn \\\n--deploy-mode client \\\n--executor-memory 1G \\\n--num-executors 5 \\\n--executor-cores 2 \\\n--conf \"spark.default.parallelism=10\" \\\ndb_spark-1.0-SNAPSHOT-jar-with-dependencies.jar\n代码块12345678910\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161909-rn4t76c","Type":"NodeParagraph","Properties":{"id":"20230817161909-rn4t76c","updated":"20230817161909"},"Children":[{"Type":"NodeText","Data":"由于修改了代码，所以需要重新编译，打包，执行"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"执行结束后再来查看spark的任务界面，可以看到此时有10 个task并行执行"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeImage","Properties":{"id":""},"Children":[{"Type":"NodeBang","Data":"!","Properties":{"id":""}},{"Type":"NodeOpenBracket","Data":"[","Properties":{"id":""}},{"Type":"NodeLinkText","Data":"图片描述","Properties":{"id":""}},{"Type":"NodeCloseBracket","Data":"]","Properties":{"id":""}},{"Type":"NodeOpenParen","Data":"(","Properties":{"id":""}},{"Type":"NodeLinkDest","Data":"https://img.mukewang.com/wiki/600973c409fba2aa19050322.jpg","Properties":{"id":""}},{"Type":"NodeCloseParen","Data":")","Properties":{"id":""}}]},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"然后点击具体的Stage，进去查看截图任务信息"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeImage","Properties":{"id":""},"Children":[{"Type":"NodeBang","Data":"!","Properties":{"id":""}},{"Type":"NodeOpenBracket","Data":"[","Properties":{"id":""}},{"Type":"NodeLinkText","Data":"图片描述","Properties":{"id":""}},{"Type":"NodeCloseBracket","Data":"]","Properties":{"id":""}},{"Type":"NodeOpenParen","Data":"(","Properties":{"id":""}},{"Type":"NodeLinkDest","Data":"https://img.mukewang.com/wiki/600973d10962be6919070326.jpg","Properties":{"id":""}},{"Type":"NodeCloseParen","Data":")","Properties":{"id":""}}]}]},{"ID":"20230817161910-gilqayb","Type":"NodeParagraph","Properties":{"id":"20230817161910-gilqayb","updated":"20230817161910"},"Children":[{"Type":"NodeText","Data":"进来之后可以看到有10个task"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeImage","Properties":{"id":""},"Children":[{"Type":"NodeBang","Data":"!","Properties":{"id":""}},{"Type":"NodeOpenBracket","Data":"[","Properties":{"id":""}},{"Type":"NodeLinkText","Data":"图片描述","Properties":{"id":""}},{"Type":"NodeCloseBracket","Data":"]","Properties":{"id":""}},{"Type":"NodeOpenParen","Data":"(","Properties":{"id":""}},{"Type":"NodeLinkDest","Data":"https://img.mukewang.com/wiki/600973ef0945ff2318971035.jpg","Properties":{"id":""}},{"Type":"NodeCloseParen","Data":")","Properties":{"id":""}}]}]},{"ID":"20230817161911-0cmgj17","Type":"NodeParagraph","Properties":{"id":"20230817161911-0cmgj17","updated":"20230817161911"},"Children":[{"Type":"NodeText","Data":"这就是并行度相关的设置"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"接下来我们来看一个图，加深一下理解"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeImage","Properties":{"id":""},"Children":[{"Type":"NodeBang","Data":"!","Properties":{"id":""}},{"Type":"NodeOpenBracket","Data":"[","Properties":{"id":""}},{"Type":"NodeLinkText","Data":"图片描述","Properties":{"id":""}},{"Type":"NodeCloseBracket","Data":"]","Properties":{"id":""}},{"Type":"NodeOpenParen","Data":"(","Properties":{"id":""}},{"Type":"NodeLinkDest","Data":"https://img.mukewang.com/wiki/5f564bd1093c8f6118710772.jpg","Properties":{"id":""}},{"Type":"NodeCloseParen","Data":")","Properties":{"id":""}}]}]},{"ID":"20230817161912-e2uix7x","Type":"NodeParagraph","Properties":{"id":"20230817161912-e2uix7x","updated":"20230817161912"},"Children":[{"Type":"NodeText","Data":"这个图中描述的就是刚才我们演示的两种情况下Executor和Task之间的关系"}]},{"ID":"20230817161913-lwx1xpi","Type":"NodeParagraph","Properties":{"id":"20230817161913-lwx1xpi","updated":"20230817161913"},"Children":[{"Type":"NodeText","Data":"最后我们来分析总结一下spark-submit脚本中经常配置的一些参数"}]},{"ID":"20230817161914-q5wjiqe","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161914-q5wjiqe","updated":"20230817161914"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"--name mySparkJobName：指定任务名称\n--class com.imooc.scala.xxxxx ：指定入口类\n--master yarn ：指定集群地址，on yarn模式指定yarn\n--deploy-mode cluster ：client代表yarn-client，cluster代表yarn-cluster\n--executor-memory 1G ：executor进程的内存大小，实际工作中设置2~4G即可\n--num-executors 2 ：分配多少个executor进程\n--executor-cores 2 : 一个executor进程分配多少个cpu core\n--driver-cores 1 ：driver进程分配多少cpu core，默认为1即可\n--driver-memory 1G：driver进程的内存，如果需要使用类似于collect之类的action算子向driver端拉取数据，则这里可以设置大一些\n--jars fastjson.jar,abc.jar 在这里可以设置job依赖的第三方jar包【不建议把第三方依赖jar包整体打包进saprk的job中，那样会导致任务jar包过大，并且也不方便统一管理依赖jar包的版本，这里的jar包路径可以指定本地磁盘路径，或者是hdfs路径，建议使用hdfs路径，因为spark在提交任务的时候会把本地磁盘的依赖jar包也上传到hdfs的一个临时目录中，如果在这里本来指定的就是hdfs的路径，那么spark在提交任务的时候就不会重复提交依赖的这个jar包了，这样其实可以提高任务提交的效率，并且这样可以方便统一管理第三方依赖jar包，所有人都统一使用hdfs中的共享的这些第三方jar包，这样版本就统一了，所以我们可以在hdfs中创建一个类似于commonLib的目录，统一存放第三方依赖的jar包，如果一个Spark job需要依赖多个jar包，在这里可以一次性指定多个，多个jar包之间通过逗号隔开即可】\n--conf \"spark.default.parallelism=10\"：可以动态指定一些spark任务的参数，指定多个参数可以通过多个--conf来指定，或者在一个--conf后面的双引号中指定多个，多个参数之间用空格隔开即可\n代码块1234567891011\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161915-x178w54","Type":"NodeParagraph","Properties":{"id":"20230817161915-x178w54","updated":"20230817161915"},"Children":[{"Type":"NodeText","Data":"最后注意一点：针对"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"--num-executors"},{"Type":"NodeText","Data":" 和"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"--executor-cores"},{"Type":"NodeText","Data":" 的设置"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"大家看这两种方式设置有什么区别："},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"第一种方式："}]},{"ID":"20230817161916-ryeqa25","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161916-ryeqa25","updated":"20230817161916"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"--num-executors 2\n--executor-cores 1\n代码块12\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161917-fmaass5","Type":"NodeParagraph","Properties":{"id":"20230817161917-fmaass5","updated":"20230817161917"},"Children":[{"Type":"NodeText","Data":"第二种方式："}]},{"ID":"20230817161918-bnv44ny","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161918-bnv44ny","updated":"20230817161918"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"--num-executors 1\n--executor-cores 2\n代码块12\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161919-r9l2haa","Type":"NodeBlockquote","Properties":{"id":"20230817161919-r9l2haa","updated":"20230817161919"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e ","Properties":{"id":""}},{"ID":"20230817161920-pl5x0m5","Type":"NodeParagraph","Properties":{"id":"20230817161920-pl5x0m5","updated":"20230817161920"},"Children":[{"Type":"NodeText","Data":"这两种设置最终都会向集群申请2个cpu core，可以并行运行两个task，但是这两种设置方式有什么区别呢？"}]}]},{"ID":"20230817161921-b11w9y9","Type":"NodeParagraph","Properties":{"id":"20230817161921-b11w9y9","updated":"20230817161921"},"Children":[{"Type":"NodeText","Data":"第一种方法：多executor模式"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"由于每个executor只分配了一个cpu core，我们将无法利用在同一个JVM中运行多个任务的优点。 我们假设这两个executor是在两个节点中启动的，那么针对广播变量这种操作，将在两个节点的中都复制1份，最终会复制两份"}]},{"ID":"20230817161922-2330oc0","Type":"NodeParagraph","Properties":{"id":"20230817161922-2330oc0","updated":"20230817161922"},"Children":[{"Type":"NodeText","Data":"第二种方法：多core模式"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"此时一个executor中会有2个cpu core，这样可以利用同一个JVM中运行多个任务的优点，并且针对广播变量的这种操作，只会在这个executor对应的节点中复制1份即可。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"那是不是我可以给一个executor分配很多的cpu core，也不是的，因为一个executor的内存大小是固定的，如果在里面运行过多的task可能会导致内存不够用，所以这块一般在工作中我们会给一个executor分配"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"2~4G"},{"Type":"NodeText","Data":"内存，对应的分配"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"2~4"},{"Type":"NodeText","Data":"个cpu core。"}]},{"ID":"20230817161923-zhis7cq","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20230817161923-zhis7cq","updated":"20230817161923"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"### ","Properties":{"id":""}},{"Type":"NodeText","Data":"数据本地化"}]},{"ID":"20230817161924-20l2i74","Type":"NodeParagraph","Properties":{"id":"20230817161924-20l2i74","updated":"20230817161924"},"Children":[{"Type":"NodeText","Data":"数据本地化，指的是，数据离计算它的代码有多近。基于数据距离代码的距离，有几种数据本地化级别："}]},{"ID":"20230817161925-1xf9sua","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161925-1xf9sua","updated":"20230817161925"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"数据本地化级别\t\t\t解释\nPROCESS_LOCAL\t\t进程本地化，性能最好：数据和计算它的代码在同一个JVM进程中\nNODE_LOCAL\t\t\t节点本地化：数据和计算它的代码在一个节点上，但是不在一个JVM进程中，数据需要跨进程传输\nNO_PREF\t\t\t\t数据从哪里过来，性能都是一样的，比如从数据库中获取数据，对于task而言没有区别\nRACK_LOCAL\t\t\t数据和计算它的代码在一个机架上，数据需要通过网络在节点之间进行传输\nANY\t\t\t\t\t数据可能在任意地方，比如其它网络环境内，或者其它机架上，性能最差\n代码块123456\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161926-no3e8sz","Type":"NodeParagraph","Properties":{"id":"20230817161926-no3e8sz","updated":"20230817161926"},"Children":[{"Type":"NodeText","Data":"Spark倾向使用最好的本地化级别调度task，但这是不现实的"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"如果目前我们要处理的数据所在的executor上目前没有空闲的CPU，那么Spark就会放低本地化级别。这时有两个选择："},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"第一，等待，直到executor上的cpu释放出来，那么就分配task过去；"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"第二，立即在任意一个其它executor上启动一个task。"}]},{"ID":"20230817161927-3gh3bfe","Type":"NodeParagraph","Properties":{"id":"20230817161927-3gh3bfe","updated":"20230817161927"},"Children":[{"Type":"NodeText","Data":"Spark默认会等待指定时间，期望task要处理的数据所在的节点上的executor空闲出一个cpu，从而将task分配过去，只要超过了时间，那么Spark就会将task分配到其它任意一个空闲的executor上"}]},{"ID":"20230817161928-fgjuq0o","Type":"NodeParagraph","Properties":{"id":"20230817161928-fgjuq0o","updated":"20230817161928"},"Children":[{"Type":"NodeText","Data":"可以设置参数，"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"spark.locality"},{"Type":"NodeText","Data":"系列参数，来调节Spark等待task可以进行数据本地化的时间"}]},{"ID":"20230817161929-tpsq49c","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161929-tpsq49c","updated":"20230817161929"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"spark.locality.wait（3000毫秒）：默认等待3秒\nspark.locality.wait.process：等待指定的时间看能否达到数据和计算它的代码在同一个JVM进程中\nspark.locality.wait.node：等待指定的时间看能否达到数据和计算它的代码在一个节点上执行\nspark.locality.wait.rack：等待指定的时间看能否达到数据和计算它的代码在一个机架上\n代码块1234\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]}]}