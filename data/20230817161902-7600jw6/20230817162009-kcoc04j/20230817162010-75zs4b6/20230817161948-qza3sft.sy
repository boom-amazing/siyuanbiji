{"ID":"20230817161948-qza3sft","Spec":"1","Type":"NodeDocument","Properties":{"id":"20230817161948-qza3sft","title":"算子优化","updated":"20230817161948"},"Children":[{"ID":"20230817161949-okaabh9","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20230817161949-okaabh9","updated":"20230817161949"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"### ","Properties":{"id":""}},{"Type":"NodeText","Data":"算子优化"}]},{"ID":"20230817161950-kdk2b6i","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20230817161950-kdk2b6i","updated":"20230817161950"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"##### ","Properties":{"id":""}},{"Type":"NodeText","Data":"Map与Mappartitions的区别"}]},{"ID":"20230817161951-du6qqfe","Type":"NodeParagraph","Properties":{"id":"20230817161951-du6qqfe","updated":"20230817161951"},"Children":[{"Type":"NodeText","Data":"map是迭代每一条数据 mappartitions是迭代一个分区的数据 针对创建数据库连接时  如果用map迭代每一条数据每次都要创建一个连接 于是我们可以用mappartitions操作 迭代一个分区 在分区里创建一个连接"}]},{"ID":"20230817161952-yv7rpg3","Type":"NodeParagraph","Properties":{"id":"20230817161952-yv7rpg3","updated":"20230817161952"},"Children":[{"Type":"NodeText","Data":"scala代码："}]},{"ID":"20230817161953-2h9kebi","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockInfo":"c2NhbGE=","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161953-2h9kebi","updated":"20230817161953"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","CodeBlockInfo":"c2NhbGE=","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"import scala.collection.mutable.ArrayBuffer\n\n/**\n * mappartitions的使用\n */\nobject MapPartitionsOpScala {\n  def main(args: Array[String]): Unit = {\n    val conf=new SparkConf()\n    conf.setMaster(\"local\").setAppName(\"MapPartitionsOpScala\")\n    val sc = new SparkContext(conf)\n    val dataRdd=sc.parallelize(Array(1,2,3,4,5),2)\n    //获取连接的代码不能写到外面 因为外面的默认是在driver端执行的 会导致连接无法序列化 无法传递给task执行\n    val sum=dataRdd.mapPartitions(it=\u003e{\n      //建议针对初始化连接之类的操作 使用mappartitions 放在mappartition内部 就放在打印那个位置\n      //例如创建数据库初始化连接 使用mappartitions可以提高性能\n      println(\"--------\")\n     val arr= new ArrayBuffer[Int]()\n      it.foreach(item=\u003e{\n        arr.+=(item*2)\n      })\n      arr.toIterator\n    }).reduce(_+_)\n    println(sum)\n  }\n}\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161954-ocxmajp","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20230817161954-ocxmajp","updated":"20230817161954"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"##### ","Properties":{"id":""}},{"Type":"NodeText","Data":"foreach与foreach"}]},{"ID":"20230817161955-sqcvbw9","Type":"NodeParagraph","Properties":{"id":"20230817161955-sqcvbw9","updated":"20230817161955"},"Children":[{"Type":"NodeText","Data":"与mappartiton类似"}]},{"ID":"20230817161956-svvwgk4","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20230817161956-svvwgk4","updated":"20230817161956"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"##### ","Properties":{"id":""}},{"Type":"NodeText","Data":"Repartition"}]},{"ID":"20230817161957-2lwevvv","Type":"NodeParagraph","Properties":{"id":"20230817161957-2lwevvv","updated":"20230817161957"},"Children":[{"Type":"NodeText","Data":"用途："}]},{"ID":"20230817161958-kb2720x","Type":"NodeParagraph","Properties":{"id":"20230817161958-kb2720x","updated":"20230817161958"},"Children":[{"Type":"NodeText","Data":"1、可以调整RDD的并行度"}]},{"ID":"20230817161959-wfww6zl","Type":"NodeParagraph","Properties":{"id":"20230817161959-wfww6zl","updated":"20230817161959"},"Children":[{"Type":"NodeText","Data":"2、重新分区可以解决数据倾斜的问题"}]},{"ID":"20230817161960-sbab8em","Type":"NodeParagraph","Properties":{"id":"20230817161960-sbab8em","updated":"20230817161960"},"Children":[{"Type":"NodeText","Data":"3、如果数据被filter后 数据量减小 我们可以在写出数据时  控制写出的文件个数 来避免让hdfs产生过多小文件"}]},{"ID":"20230817161961-9nxvtzh","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockInfo":"c2NhbGE=","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161961-9nxvtzh","updated":"20230817161961"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","CodeBlockInfo":"c2NhbGE=","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"object RepartitionUse {\n  def main(args: Array[String]): Unit = {\n    val conf=new SparkConf()\n    conf.setMaster(\"local\").setAppName(\"RepartitionUse \")\n    val sc = new SparkContext(conf)\n    val datardd=sc.parallelize(Array(1,2,3,4,5),2)\n    //重新分区可以解决数据倾斜的问题\n    datardd.repartition(3)\n      .foreachPartition(it=\u003e{\n        println(\"------\")\n        it.foreach(println(_))\n      })\n    //如果数据被filter后 数据量减小 我们可以在写出数据时  控制写出的文件个数 来避免让hdfs产生过多小文件\n    datardd.saveAsTextFile(\"hdfs://bigdata01:9000/rep-001\")\n    datardd.repartition(1).saveAsTextFile(\"hdfs://bigdata01:9000/rep-002\");\n  }\n\n}\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161962-cenhs28","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20230817161962-cenhs28","updated":"20230817161962"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"##### ","Properties":{"id":""}},{"Type":"NodeText","Data":"ReduceBykey与groupbykey的区别"}]},{"ID":"20230817161963-b2hkgjx","Type":"NodeParagraph","Properties":{"id":"20230817161963-b2hkgjx","updated":"20230817161963"},"Children":[{"Type":"NodeText","Data":"他们两个在分组聚合时 都会产生shuffle"}]},{"ID":"20230817161964-cw2qnl8","Type":"NodeParagraph","Properties":{"id":"20230817161964-cw2qnl8","updated":"20230817161964"},"Children":[{"Type":"NodeText","Data":"reducebykey 在分组前每个分区会产生一个局部聚合 shuffle后再进行总体聚合"}]},{"ID":"20230817161965-3u1pqoa","Type":"NodeParagraph","Properties":{"id":"20230817161965-3u1pqoa","updated":"20230817161965"},"Children":[{"Type":"NodeText","Data":"而groupbykey会直接进行shuffle 在shuffle后再总体聚合"}]},{"ID":"20230817161966-2wjetnp","Type":"NodeParagraph","Properties":{"id":"20230817161966-2wjetnp","updated":"20230817161966"},"Children":[{"Type":"NodeText","Data":"下面来看这个图，加深一下理解"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeImage","Properties":{"id":""},"Children":[{"Type":"NodeBang","Data":"!","Properties":{"id":""}},{"Type":"NodeOpenBracket","Data":"[","Properties":{"id":""}},{"Type":"NodeLinkText","Data":"图片描述","Properties":{"id":""}},{"Type":"NodeCloseBracket","Data":"]","Properties":{"id":""}},{"Type":"NodeOpenParen","Data":"(","Properties":{"id":""}},{"Type":"NodeLinkDest","Data":"https://img.mukewang.com/wiki/5f564d01095bb1ce18860762.jpg","Properties":{"id":""}},{"Type":"NodeCloseParen","Data":")","Properties":{"id":""}}]}]},{"ID":"20230817161967-a5ygrxg","Type":"NodeParagraph","Properties":{"id":"20230817161967-a5ygrxg","updated":"20230817161967"},"Children":[{"Type":"NodeText","Data":"从图中可以看出来reduceByKey在shuffle之前会先对数据进行局部聚合，而groupByKey不会，所以在实现分组聚合的需求中，reduceByKey性能略胜一筹。"}]},{"ID":"20230817161968-e3v2qtk","Type":"NodeParagraph","Properties":{"id":"20230817161968-e3v2qtk","updated":"20230817161968"},"Children":[{"Type":"NodeText","Data":"两种方式实现word计数的代码"}]},{"ID":"20230817161969-bltxb0o","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockInfo":"c2NhbGE=","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161969-bltxb0o","updated":"20230817161969"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","CodeBlockInfo":"c2NhbGE=","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"object ReduceBykey {\n  def main(args: Array[String]): Unit = {\n    val conf=new SparkConf()\n    conf.setMaster(\"local\").setAppName(\"ReduceBykey\")\n    val sc = new SparkContext(conf)\n    val dataRDD = sc.parallelize(Array(\"hello you\", \"hello me\"))\n//   dataRDD.flatMap(_.split(\" \")).map((_,1)).reduceByKey(_+_).foreach(println(_))\n    dataRDD.flatMap(_.split(\" \")).map((_,1)).groupByKey().map(word=\u003e{(word._1,word._2.sum)}).\n      foreach(println(_))\n    sc.stop()\n  }\n}\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]}]}