{"ID":"20230817161907-e3e7wli","Spec":"1","Type":"NodeDocument","Properties":{"id":"20230817161907-e3e7wli","title":"Spark的工作原理","updated":"20230817161907"},"Children":[{"ID":"20230817161908-aijdp35","Type":"NodeHeading","HeadingLevel":4,"Properties":{"id":"20230817161908-aijdp35","updated":"20230817161908"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"#### ","Properties":{"id":""}},{"Type":"NodeText","Data":"spark工作与架构原理"}]},{"ID":"20230817161909-qwzltpk","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20230817161909-qwzltpk","updated":"20230817161909"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"## ","Properties":{"id":""}},{"Type":"NodeText","Data":"Spark的工作原理"}]},{"ID":"20230817161910-2fsk4h1","Type":"NodeParagraph","Properties":{"id":"20230817161910-2fsk4h1","updated":"20230817161910"},"Children":[{"Type":"NodeText","Data":"下面我们来分析一下Spark的工作原理"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"来看这个图"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeImage","Properties":{"id":""},"Children":[{"Type":"NodeBang","Data":"!","Properties":{"id":""}},{"Type":"NodeOpenBracket","Data":"[","Properties":{"id":""}},{"Type":"NodeLinkText","Data":"图片描述","Properties":{"id":""}},{"Type":"NodeCloseBracket","Data":"]","Properties":{"id":""}},{"Type":"NodeOpenParen","Data":"(","Properties":{"id":""}},{"Type":"NodeLinkDest","Data":"https://img.mukewang.com/wiki/5f51ba910913851a17730831.jpg","Properties":{"id":""}},{"Type":"NodeCloseParen","Data":")","Properties":{"id":""}}]}]},{"ID":"20230817161911-lpynuse","Type":"NodeParagraph","Properties":{"id":"20230817161911-lpynuse","updated":"20230817161911"},"Children":[{"Type":"NodeText","Data":"首先看中间是一个Spark集群，可以理解为是Spark的 standalone集群，集群中有6个节点"}]},{"ID":"20230817161912-ur379c9","Type":"NodeParagraph","Properties":{"id":"20230817161912-ur379c9","updated":"20230817161912"},"Children":[{"Type":"NodeText","Data":"左边是Spark的客户端节点，这个节点主要负责向Spark集群提交任务，假设在这里我们向Spark集群提交了一个任务"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"那这个Spark任务肯定会有一个数据源，数据源在这我们使用HDFS，就是让Spark计算HDFS中的数据。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"当Spark任务把HDFS中的数据读取出来之后，它会把HDFS中的数据转化为RDD，RDD其实是一个弹性分布式数据集，它其实是一个逻辑概念，在这你先把它理解为是一个数据集合就可以了，后面我们会详细分析这个RDD。"}]},{"ID":"20230817161913-gbpyxzu","Type":"NodeParagraph","Properties":{"id":"20230817161913-gbpyxzu","updated":"20230817161913"},"Children":[{"Type":"NodeText","Data":"在这里这个RDD你就可以认为是包含了我们读取的HDFS上的数据"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"其中这个RDD是有分区这个特性的，也就是一整份数据会被分成多份，"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"假设我们现在从HDFS中读取的这份数据被转化为RDD之后，在RDD中分成了3份，那这3份数据可能会分布在3个不同的节点上面，对应这里面的节点1、节点2、节点3"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"这个RDD的3个分区的数据对应的是partiton-1、partition-2、partition-3"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"这样的好处是可以并行处理了，后期每个节点就可以计算当前节点上的这一个分区的数据。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"这个计算思想是不是类似于MapReduce里面的计算思想啊，本地计算，但是有一点区别就是这个RDD的数据是在内存中的。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"假设现在这个RDD中每个分区中的数据有10w条"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"那接下来我们就想对这个RDD中的数据进行计算了，可以使用一些高阶函数进行计算，例如：flatMap、map之类的"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"那在这我们先使用flatMap对数据进行处理，把每一行数据转成多行数据"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"此时flatMap这个函数就会在节点1、节点2和节点3上并行执行了。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"计算之后的结果还是一个带有分区的RDD，那这个RDD我们假设存在节点4、节点5和节点6上面。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"此时每个节点上面会有一个分区的数据，我们给这些分区数据起名叫partition-4、partition-5、partition-6"}]},{"ID":"20230817161914-10y4q5t","Type":"NodeParagraph","Properties":{"id":"20230817161914-10y4q5t","updated":"20230817161914"},"Children":[{"Type":"NodeText","Data":"正常情况下，前面节点1上的数据处理之后会发送到节点4上面"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"另外两个节点也是一样的。"}]},{"ID":"20230817161915-jfncdf8","Type":"NodeParagraph","Properties":{"id":"20230817161915-jfncdf8","updated":"20230817161915"},"Children":[{"Type":"NodeText","Data":"此时经过flatmap计算之后，前面RDD的数据传输到后面节点上面这个过程是不需要经过shuffle的，可以直接在内存中通过网络传输过去，因为现在这两个RDD的分区数量是一一对应的。"}]},{"ID":"20230817161916-ekdb46m","Type":"NodeParagraph","Properties":{"id":"20230817161916-ekdb46m","updated":"20230817161916"},"Children":[{"Type":"NodeText","Data":"后面可能还会通过map、或者其它的一些高阶函数对数据进行处理，当处理到最后一步的时候是需要把数据存储起来的，在这我们选择把数据存储到hdfs上面，其实在实际工作中，针对这种离线计算，大部分的结果数据都是存储在hdfs上面的，当然了也可以存储到其它的存储介质中。"}]},{"ID":"20230817161917-9lwpdi2","Type":"NodeParagraph","Properties":{"id":"20230817161917-9lwpdi2","updated":"20230817161917"},"Children":[{"Type":"NodeText","Data":"好，那这个就是Spark的基本工作原理。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"再梳理一下，首先通过Spark客户端提交任务到Spark集群，然后Spark任务在执行的时候会读取数据源HDFS中的数据，将数据加载到内存中，转化为RDD，然后针对RDD调用一些高阶函数对数据进行处理，中间可以调用多个高阶函数，最终把计算出来的结果数据写到HDFS中。"}]},{"ID":"20230817161918-l41ac53","Type":"NodeParagraph","Properties":{"id":"20230817161918-l41ac53","updated":"20230817161918"},"Children":[{"Type":"NodeText","Data":"这里面的这个RDD是Spark的核心内容，那下面我们来详细分析一下这个RDD"}]},{"ID":"20230817161919-48083vq","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20230817161919-48083vq","updated":"20230817161919"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"## ","Properties":{"id":""}},{"Type":"NodeText","Data":"什么是RDD"}]},{"ID":"20230817161920-stvkfwq","Type":"NodeParagraph","Properties":{"id":"20230817161920-stvkfwq","updated":"20230817161920"},"Children":[{"Type":"NodeText","Data":"RDD通常通过Hadoop上的文件，即HDFS文件进行创建，也可以通过程序中的集合来创建"}]},{"ID":"20230817161921-h7iwgvm","Type":"NodeParagraph","Properties":{"id":"20230817161921-h7iwgvm","updated":"20230817161921"},"Children":[{"Type":"NodeText","Data":"RDD是Spark提供的核心抽象，全称为Resillient Distributed Dataset，即弹性分布式数据集"}]},{"ID":"20230817161922-02qmx1k","Type":"NodeParagraph","Properties":{"id":"20230817161922-02qmx1k","updated":"20230817161922"},"Children":[{"Type":"NodeText","Data":"那我们接下来来看一下这个弹性分布式数据集的特点"}]},{"ID":"20230817161923-95ow3fn","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20230817161923-95ow3fn","updated":"20230817161923"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"## ","Properties":{"id":""}},{"Type":"NodeText","Data":"RDD的特点"}]},{"ID":"20230817161924-zxx501p","Type":"NodeList","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161924-zxx501p","updated":"20230817161924"},"Children":[{"ID":"20230817161925-z4324dg","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161925-z4324dg","updated":"20230817161925"},"Children":[{"ID":"20230817161926-az9izab","Type":"NodeParagraph","Properties":{"id":"20230817161926-az9izab","updated":"20230817161926"},"Children":[{"Type":"NodeText","Data":"弹性：RDD数据默认情况下存放在内存中，但是在内存资源不足时，Spark也会自动将RDD数据写入磁盘"}]}]},{"ID":"20230817161927-026efia","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161927-026efia","updated":"20230817161927"},"Children":[{"ID":"20230817161928-8voqchd","Type":"NodeParagraph","Properties":{"id":"20230817161928-8voqchd","updated":"20230817161928"},"Children":[{"Type":"NodeText","Data":"分布式：RDD在抽象上来说是一种元素数据的集合，它是被分区的，每个分区分布在集群中的不同节点上，从而让RDD中的数据可以被并行操作"}]}]},{"ID":"20230817161929-ipothzx","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161929-ipothzx","updated":"20230817161929"},"Children":[{"ID":"20230817161930-uupjx43","Type":"NodeParagraph","Properties":{"id":"20230817161930-uupjx43","updated":"20230817161930"},"Children":[{"Type":"NodeText","Data":"容错性：RDD最重要的特性就是提供了容错性，可以自动从节点失败中恢复过来"}]}]}]},{"ID":"20230817161931-r7gkwge","Type":"NodeParagraph","Properties":{"id":"20230817161931-r7gkwge","updated":"20230817161931"},"Children":[{"Type":"NodeText","Data":"如果某个节点上的RDD partition，因为节点故障，导致数据丢了，那么RDD会自动通过自己的数据来源重新计算该partition的数据。"}]},{"ID":"20230817161932-urrl533","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20230817161932-urrl533","updated":"20230817161932"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"## ","Properties":{"id":""}},{"Type":"NodeText","Data":"Spark架构相关进程"}]},{"ID":"20230817161933-z05ec7q","Type":"NodeParagraph","Properties":{"id":"20230817161933-z05ec7q","updated":"20230817161933"},"Children":[{"Type":"NodeText","Data":"下面我们来看一下Spark架构相关的进程信息"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"注意：在这里是以Spark的standalone集群为例进行分析"}]},{"ID":"20230817161934-56uvbgt","Type":"NodeList","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161934-56uvbgt","updated":"20230817161934"},"Children":[{"ID":"20230817161935-ienfb7s","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161935-ienfb7s","updated":"20230817161935"},"Children":[{"ID":"20230817161936-nxfpryc","Type":"NodeParagraph","Properties":{"id":"20230817161936-nxfpryc","updated":"20230817161936"},"Children":[{"Type":"NodeText","Data":"Driver："}]}]}]},{"ID":"20230817161937-5knvkr5","Type":"NodeParagraph","Properties":{"id":"20230817161937-5knvkr5","updated":"20230817161937"},"Children":[{"Type":"NodeText","Data":"我们编写的Spark程序就在Driver(进程)上，由Driver进程负责执行"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"Driver进程所在的节点可以是Spark集群的某一个节点或者就是我们提交Spark程序的客户端节点"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"具体Driver进程在哪个节点上启动是由我们提交任务时指定的参数决定的，这个后面我们会详细分析"}]},{"ID":"20230817161938-yo9ulpu","Type":"NodeList","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161938-yo9ulpu","updated":"20230817161938"},"Children":[{"ID":"20230817161939-v6vv8fj","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161939-v6vv8fj","updated":"20230817161939"},"Children":[{"ID":"20230817161940-h7e08ly","Type":"NodeParagraph","Properties":{"id":"20230817161940-h7e08ly","updated":"20230817161940"},"Children":[{"Type":"NodeText","Data":"Master："}]}]}]},{"ID":"20230817161941-ii38qb9","Type":"NodeParagraph","Properties":{"id":"20230817161941-ii38qb9","updated":"20230817161941"},"Children":[{"Type":"NodeText","Data":"集群的主节点中启动的进程"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"主要负责集群资源的管理和分配，还有集群的监控等"}]},{"ID":"20230817161942-cng26y1","Type":"NodeList","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161942-cng26y1","updated":"20230817161942"},"Children":[{"ID":"20230817161943-0qsxgel","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161943-0qsxgel","updated":"20230817161943"},"Children":[{"ID":"20230817161944-25evmkg","Type":"NodeParagraph","Properties":{"id":"20230817161944-25evmkg","updated":"20230817161944"},"Children":[{"Type":"NodeText","Data":"Worker："}]}]}]},{"ID":"20230817161945-0wjaqd8","Type":"NodeParagraph","Properties":{"id":"20230817161945-0wjaqd8","updated":"20230817161945"},"Children":[{"Type":"NodeText","Data":"集群的从节点中启动的进程"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"主要负责启动其它进程来执行具体数据的处理和计算任务"}]},{"ID":"20230817161946-2k4kbkp","Type":"NodeList","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161946-2k4kbkp","updated":"20230817161946"},"Children":[{"ID":"20230817161947-um3cmko","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161947-um3cmko","updated":"20230817161947"},"Children":[{"ID":"20230817161948-39mbv21","Type":"NodeParagraph","Properties":{"id":"20230817161948-39mbv21","updated":"20230817161948"},"Children":[{"Type":"NodeText","Data":"Executor："}]}]}]},{"ID":"20230817161949-eqjujhr","Type":"NodeParagraph","Properties":{"id":"20230817161949-eqjujhr","updated":"20230817161949"},"Children":[{"Type":"NodeText","Data":"此进程由Worker负责启动，主要为了执行数据处理和计算"}]},{"ID":"20230817161950-ogq3owf","Type":"NodeList","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161950-ogq3owf","updated":"20230817161950"},"Children":[{"ID":"20230817161951-611sm67","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161951-611sm67","updated":"20230817161951"},"Children":[{"ID":"20230817161952-5q3x18c","Type":"NodeParagraph","Properties":{"id":"20230817161952-5q3x18c","updated":"20230817161952"},"Children":[{"Type":"NodeText","Data":"Task："}]}]}]},{"ID":"20230817161953-r8k0pa0","Type":"NodeParagraph","Properties":{"id":"20230817161953-r8k0pa0","updated":"20230817161953"},"Children":[{"Type":"NodeText","Data":"是一个线程"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"由Executor负责启动，它是真正干活的"}]},{"ID":"20230817161954-u8uddl0","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20230817161954-u8uddl0","updated":"20230817161954"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"## ","Properties":{"id":""}},{"Type":"NodeText","Data":"Spark架构原理"}]},{"ID":"20230817161955-dbn311o","Type":"NodeParagraph","Properties":{"id":"20230817161955-dbn311o","updated":"20230817161955"},"Children":[{"Type":"NodeText","Data":"下面来看一个图，通过刚才那几个进程，我们来分析一下Spark的架构原理"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeImage","Properties":{"id":""},"Children":[{"Type":"NodeBang","Data":"!","Properties":{"id":""}},{"Type":"NodeOpenBracket","Data":"[","Properties":{"id":""}},{"Type":"NodeLinkText","Data":"图片描述","Properties":{"id":""}},{"Type":"NodeCloseBracket","Data":"]","Properties":{"id":""}},{"Type":"NodeOpenParen","Data":"(","Properties":{"id":""}},{"Type":"NodeLinkDest","Data":"https://img.mukewang.com/wiki/5f51bae40961c7d312680626.jpg","Properties":{"id":""}},{"Type":"NodeCloseParen","Data":")","Properties":{"id":""}}]}]},{"ID":"20230817161956-rp14a2a","Type":"NodeList","ListData":{"Typ":1,"Tight":true,"Start":1,"Delimiter":46,"Padding":3,"Marker":"MQ==","Num":1},"Properties":{"id":"20230817161956-rp14a2a","updated":"20230817161956"},"Children":[{"ID":"20230817161957-91pf6ea","Type":"NodeListItem","Data":"1","ListData":{"Typ":1,"Tight":true,"Start":1,"Delimiter":46,"Padding":3,"Marker":"MQ==","Num":1},"Properties":{"id":"20230817161957-91pf6ea","updated":"20230817161957"},"Children":[{"ID":"20230817161958-cozw8eo","Type":"NodeParagraph","Properties":{"id":"20230817161958-cozw8eo","updated":"20230817161958"},"Children":[{"Type":"NodeText","Data":"首先我们在spark的客户端机器上通过driver进程执行我们的Spark代码"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"当我们通过spark-submit脚本提交Spark任务的时候Driver进程就启动了。"}]}]},{"ID":"20230817161959-y4j8jmr","Type":"NodeListItem","Data":"2","ListData":{"Typ":1,"Tight":true,"Start":2,"Delimiter":46,"Padding":3,"Marker":"Mg==","Num":2},"Properties":{"id":"20230817161959-y4j8jmr","updated":"20230817161959"},"Children":[{"ID":"20230817161960-kydu3b2","Type":"NodeParagraph","Properties":{"id":"20230817161960-kydu3b2","updated":"20230817161960"},"Children":[{"Type":"NodeText","Data":"Driver进程启动之后，会做一些初始化的操作，会找到集群master进程，对Spark应用程序进行注册"}]}]},{"ID":"20230817161961-c5wa34c","Type":"NodeListItem","Data":"3","ListData":{"Typ":1,"Tight":true,"Start":3,"Delimiter":46,"Padding":3,"Marker":"Mw==","Num":3},"Properties":{"id":"20230817161961-c5wa34c","updated":"20230817161961"},"Children":[{"ID":"20230817161962-438r3vl","Type":"NodeParagraph","Properties":{"id":"20230817161962-438r3vl","updated":"20230817161962"},"Children":[{"Type":"NodeText","Data":"当Master收到Spark程序的注册申请之后，会发送请求给Worker，进行资源的调度和分配"}]}]},{"ID":"20230817161963-hl559o9","Type":"NodeListItem","Data":"4","ListData":{"Typ":1,"Tight":true,"Start":4,"Delimiter":46,"Padding":3,"Marker":"NA==","Num":4},"Properties":{"id":"20230817161963-hl559o9","updated":"20230817161963"},"Children":[{"ID":"20230817161964-xlds5il","Type":"NodeParagraph","Properties":{"id":"20230817161964-xlds5il","updated":"20230817161964"},"Children":[{"Type":"NodeText","Data":"Worker收到Master的请求之后，会为Spark应用启动Executor进程"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"会启动一个或者多个Executor，具体启动多少个，会根据你的配置来启动"}]}]},{"ID":"20230817161965-y3ahyyj","Type":"NodeListItem","Data":"5","ListData":{"Typ":1,"Tight":true,"Start":5,"Delimiter":46,"Padding":3,"Marker":"NQ==","Num":5},"Properties":{"id":"20230817161965-y3ahyyj","updated":"20230817161965"},"Children":[{"ID":"20230817161966-qvhn0de","Type":"NodeParagraph","Properties":{"id":"20230817161966-qvhn0de","updated":"20230817161966"},"Children":[{"Type":"NodeText","Data":"Executor启动之后，会向Driver进行反注册，这样Driver就知道哪些Executor在为它服务了"}]}]},{"ID":"20230817161967-mleqjap","Type":"NodeListItem","Data":"6","ListData":{"Typ":1,"Tight":true,"Start":6,"Delimiter":46,"Padding":3,"Marker":"Ng==","Num":6},"Properties":{"id":"20230817161967-mleqjap","updated":"20230817161967"},"Children":[{"ID":"20230817161968-iuwz3n5","Type":"NodeParagraph","Properties":{"id":"20230817161968-iuwz3n5","updated":"20230817161968"},"Children":[{"Type":"NodeText","Data":"Driver会根据我们对RDD定义的操作，提交一堆的task去Executor上执行"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"task里面执行的其实就是具体的map、flatMap这些操作。"}]}]}]},{"ID":"20230817161969-hgyrz8z","Type":"NodeParagraph","Properties":{"id":"20230817161969-hgyrz8z","updated":"20230817161969"},"Children":[{"Type":"NodeText","Data":"这就是Spark架构的原理。"}]}]}