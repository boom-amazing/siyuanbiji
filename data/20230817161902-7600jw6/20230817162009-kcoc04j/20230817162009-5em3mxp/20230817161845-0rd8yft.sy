{"ID":"20230817161845-0rd8yft","Spec":"1","Type":"NodeDocument","Properties":{"id":"20230817161845-0rd8yft","title":"Kafka-Connector","updated":"20230817161845"},"Children":[{"ID":"20230817161846-a7lfzmj","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20230817161846-a7lfzmj","updated":"20230817161846"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"## ","Properties":{"id":""}},{"Type":"NodeText","Data":"Kafka-Connector"}]},{"ID":"20230817161847-9a0sqn3","Type":"NodeParagraph","Properties":{"id":"20230817161847-9a0sqn3","updated":"20230817161847"},"Children":[{"Type":"NodeText","Data":"针对Flink的流处理，最常用的组件就是Kafka，原始日志数据产生后会被日志采集工具采集到Kafka中让Flink去处理，处理之后的数据可能也会继续写入到Kafka中，Kafka可以作为Flink的DataSource和DataSink来使用"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"并且Kafka中的Partition机制和Flink的并行度机制可以深度结合，提高数据的读取效率和写入效率。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"想要在Flink中使用Kafka需要添加对应的依赖"}]},{"ID":"20230817161848-gfdjo36","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161848-gfdjo36","updated":"20230817161848"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"\u003cdependency\u003e\n    \u003cgroupId\u003eorg.apache.flink\u003c/groupId\u003e\n    \u003cartifactId\u003eflink-connector-kafka_2.12\u003c/artifactId\u003e\n    \u003cversion\u003e1.11.1\u003c/version\u003e\n\u003c/dependency\u003e\n代码块12345\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161849-bakt79w","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20230817161849-bakt79w","updated":"20230817161849"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"## ","Properties":{"id":""}},{"Type":"NodeText","Data":"Kafka Consumer的使用"}]},{"ID":"20230817161850-2xc3pxh","Type":"NodeParagraph","Properties":{"id":"20230817161850-2xc3pxh","updated":"20230817161850"},"Children":[{"Type":"NodeText","Data":"我们演示一下在Flink中如何消费Kafka中的数据，此时需要用到Kafka Consumer"}]},{"ID":"20230817161851-sgxolc7","Type":"NodeParagraph","Properties":{"id":"20230817161851-sgxolc7","updated":"20230817161851"},"Children":[{"Type":"NodeText","Data":"scala代码如下："}]},{"ID":"20230817161852-o9a64rt","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161852-o9a64rt","updated":"20230817161852"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"package com.imooc.scala.kafkaconnector\n\nimport java.util.Properties\n\nimport org.apache.flink.api.common.serialization.SimpleStringSchema\nimport org.apache.flink.streaming.api.scala.StreamExecutionEnvironment\nimport org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer\n\n/**\n * Flink从Kafka中消费数据\n * Created by xuwei\n */\nobject StreamKafkaSourceScala {\n  def main(args: Array[String]): Unit = {\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n\n    //指定FlinkKafkaConsumer相关配置\n    val topic = \"t1\"\n    val prop = new Properties()\n    prop.setProperty(\"bootstrap.servers\",\"bigdata01:9092,bigdata02:9092,bigdata03:9092\")\n    prop.setProperty(\"group.id\",\"con1\")\n    val kafkaConsumer = new FlinkKafkaConsumer[String](topic, new SimpleStringSchema(), prop)\n\n    //指定Kafka作为Source\n    import org.apache.flink.api.scala._\n    val text = env.addSource(kafkaConsumer)\n\n    //将读取到的数据打印到控制台上\n    text.print()\n\n    env.execute(\"StreamKafkaSourceScala\")\n  }\n}\n代码块123456789101112131415161718192021222324252627282930313233\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161853-4ac9mv6","Type":"NodeParagraph","Properties":{"id":"20230817161853-4ac9mv6","updated":"20230817161853"},"Children":[{"Type":"NodeText","Data":"在运行代码之前，需要先启动zookeeper集群和kafka集群"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"在kafka中创建topic：t1"}]},{"ID":"20230817161854-25rovoh","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161854-25rovoh","updated":"20230817161854"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"[root@bigdata01 kafka_2.12-2.4.1]# bin/kafka-topics.sh --create --zookeeper localhost:2181 --partitions 5 --replication-factor 2 --topic t1\n代码块1\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161855-7vtqelt","Type":"NodeParagraph","Properties":{"id":"20230817161855-7vtqelt","updated":"20230817161855"},"Children":[{"Type":"NodeText","Data":"然后启动代码"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"再启动一个Kafka 的 console生产者模拟产生数据，验证效果。"}]},{"ID":"20230817161856-uwirev1","Type":"NodeParagraph","Properties":{"id":"20230817161856-uwirev1","updated":"20230817161856"},"Children":[{"Type":"NodeText","Data":"java代码如下："}]},{"ID":"20230817161857-h7rgceo","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161857-h7rgceo","updated":"20230817161857"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"package com.imooc.java.kafkaconnector;\n\nimport org.apache.flink.api.common.serialization.SimpleStringSchema;\nimport org.apache.flink.streaming.api.datastream.DataStreamSource;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\nimport org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;\n\nimport java.util.Properties;\n\n/**\n * Flink从Kafka中消费数据\n * Created by xuwei\n */\npublic class StreamKafkaSourceJava {\n    public static void main(String[] args) throws Exception{\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n        //指定FlinkKafkaConsumer相关配置\n        String topic = \"t1\";\n        Properties prop = new Properties();\n        prop.setProperty(\"bootstrap.servers\",\"bigdata01:9092,bigdata02:9092,bigdata03:9092\");\n        prop.setProperty(\"group.id\",\"con1\");\n        FlinkKafkaConsumer\u003cString\u003e kafkaConsumer = new FlinkKafkaConsumer\u003c\u003e(topic, new SimpleStringSchema(), prop);\n\n        //指定Kafka作为Source\n        DataStreamSource\u003cString\u003e text = env.addSource(kafkaConsumer);\n\n        //将读取到的数据打印到控制台上\n        text.print();\n\n        env.execute(\"StreamKafkaSourceJava\");\n    }\n}\n代码块123456789101112131415161718192021222324252627282930313233\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161858-b6fwvr3","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20230817161858-b6fwvr3","updated":"20230817161858"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"## ","Properties":{"id":""}},{"Type":"NodeText","Data":"Kafka Consumer消费策略设置"}]},{"ID":"20230817161859-vuq0sp2","Type":"NodeParagraph","Properties":{"id":"20230817161859-vuq0sp2","updated":"20230817161859"},"Children":[{"Type":"NodeText","Data":"针对Kafka Consumer消费数据的时候会有一些策略，我们来看一下"}]},{"ID":"20230817161860-hbu7291","Type":"NodeList","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161860-hbu7291","updated":"20230817161860"},"Children":[{"ID":"20230817161861-q647dt4","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161861-q647dt4","updated":"20230817161861"},"Children":[{"ID":"20230817161862-vb8l62p","Type":"NodeParagraph","Properties":{"id":"20230817161862-vb8l62p","updated":"20230817161862"},"Children":[{"Type":"NodeText","Data":"setStartFromGroupOffsets()【默认消费策略】"}]}]},{"ID":"20230817161863-9ngrw1t","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161863-9ngrw1t","updated":"20230817161863"},"Children":[{"ID":"20230817161864-wumtust","Type":"NodeParagraph","Properties":{"id":"20230817161864-wumtust","updated":"20230817161864"},"Children":[{"Type":"NodeText","Data":"setStartFromEarliest() 或者 setStartFromLatest()"}]}]},{"ID":"20230817161865-zcmk6uw","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161865-zcmk6uw","updated":"20230817161865"},"Children":[{"ID":"20230817161866-647c30a","Type":"NodeParagraph","Properties":{"id":"20230817161866-647c30a","updated":"20230817161866"},"Children":[{"Type":"NodeText","Data":"setStartFromTimestamp(…)"}]}]}]},{"ID":"20230817161867-3yw2gjt","Type":"NodeParagraph","Properties":{"id":"20230817161867-3yw2gjt","updated":"20230817161867"},"Children":[{"Type":"NodeText","Data":"代码如下："}]},{"ID":"20230817161868-vuccrkw","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161868-vuccrkw","updated":"20230817161868"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"//kafka consumer的消费策略设置\n//默认策略，读取group.id对应保存的offset开始消费数据，读取不到则根据kafka中auto.offset.reset参数的值开始消费数据\nkafkaConsumer.setStartFromGroupOffsets()\n//从最早的记录开始消费数据，忽略已提交的offset信息\n//kafkaConsumer.setStartFromEarliest()\n//从最新的记录开始消费数据，忽略已提交的offset信息\n//kafkaConsumer.setStartFromLatest()\n//从指定的时间戳开始消费数据，对于每个分区，其时间戳大于或等于指定时间戳的记录将被作为起始位置\n//kafkaConsumer.setStartFromTimestamp(1769498624)\n代码块123456789\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161869-w9vidv9","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20230817161869-w9vidv9","updated":"20230817161869"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"## ","Properties":{"id":""}},{"Type":"NodeText","Data":"Kafka Consumer的容错"}]},{"ID":"20230817161870-qgwih9f","Type":"NodeParagraph","Properties":{"id":"20230817161870-qgwih9f","updated":"20230817161870"},"Children":[{"Type":"NodeText","Data":"Flink中也有checkpoint机制，Checkpoint是Flink实现容错机制的核心功能，它能够根据配置周期性地基于流中各个算子任务的State来生成快照，从而将这些State数据定期持久化存储下来，当Flink程序一旦意外崩溃时，重新运行程序时可以有选择地从这些快照进行恢复，从而修正因为故障带来的程序数据异常。"}]},{"ID":"20230817161871-9n3hpjf","Type":"NodeParagraph","Properties":{"id":"20230817161871-9n3hpjf","updated":"20230817161871"},"Children":[{"Type":"NodeText","Data":"当CheckPoint机制开启的时候，Consumer会定期把Kafka的offset信息还有其它算子任务的State信息一块保存起来"}]},{"ID":"20230817161872-9j0me8k","Type":"NodeParagraph","Properties":{"id":"20230817161872-9j0me8k","updated":"20230817161872"},"Children":[{"Type":"NodeText","Data":"当Job失败重启的时候，Flink会从最近一次的CheckPoint中进行恢复数据，重新消费Kafka中的数据"}]},{"ID":"20230817161873-2ghdx9d","Type":"NodeParagraph","Properties":{"id":"20230817161873-2ghdx9d","updated":"20230817161873"},"Children":[{"Type":"NodeText","Data":"为了能够使用支持容错的Consumer，需要开启checkpoint"}]},{"ID":"20230817161874-4y4kqfp","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20230817161874-4y4kqfp","updated":"20230817161874"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"## ","Properties":{"id":""}},{"Type":"NodeText","Data":"如何开启Checkpoint"}]},{"ID":"20230817161875-0yf8kqs","Type":"NodeParagraph","Properties":{"id":"20230817161875-0yf8kqs","updated":"20230817161875"},"Children":[{"Type":"NodeText","Data":"那如何开启checkpoint呢？"}]},{"ID":"20230817161876-u7urnkx","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161876-u7urnkx","updated":"20230817161876"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"//每隔5000 ms执行一次Checkpoint(设置Checkpoint的周期)\nenv.enableCheckpointing(5000)\n代码块12\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161877-upxef8w","Type":"NodeParagraph","Properties":{"id":"20230817161877-upxef8w","updated":"20230817161877"},"Children":[{"Type":"NodeText","Data":"针对checkpoint还有一些相关的配置"}]},{"ID":"20230817161878-xbfebiu","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161878-xbfebiu","updated":"20230817161878"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"//设置模式为.EXACTLY_ONCE (这是默认值) ,还可以设置为AT_LEAST_ONCE\nenv.getCheckpointConfig.setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE)\n//确保两次Checkpoint之间有至少多少 ms的间隔(checkpoint最小间隔)\nenv.getCheckpointConfig.setMinPauseBetweenCheckpoints(500)\n//Checkpoint必须在一分钟内完成，或者被丢弃(checkpoint的超时时间)\nenv.getCheckpointConfig.setCheckpointTimeout(60000)\n//同一时间只允许执行一个Checkpoint\nenv.getCheckpointConfig.setMaxConcurrentCheckpoints(1)\n//表示一旦Flink处理程序被cancel后，会保留Checkpoint数据，以便根据实际需要恢复到指定的Checkpoint\nenv.getCheckpointConfig.enableExternalizedCheckpoints(CheckpointConfig.ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION)\n代码块12345678910\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161879-gbtp5j9","Type":"NodeParagraph","Properties":{"id":"20230817161879-gbtp5j9","updated":"20230817161879"},"Children":[{"Type":"NodeText","Data":"最后还有一个配置，设置State数据存储的位置"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"默认情况下，State数据会保存在TaskManager的内存中，Checkpoint执行时，会将State数据存储在JobManager的内存中。"}]},{"ID":"20230817161880-iuwy9i9","Type":"NodeParagraph","Properties":{"id":"20230817161880-iuwy9i9","updated":"20230817161880"},"Children":[{"Type":"NodeText","Data":"具体的存储位置取决于State Backend的配置，Flink 一共提供了3种存储方式"}]},{"ID":"20230817161881-8x0213x","Type":"NodeList","ListData":{"Typ":1,"Tight":true,"Start":1,"Delimiter":46,"Padding":3,"Marker":"MQ==","Num":1},"Properties":{"id":"20230817161881-8x0213x","updated":"20230817161881"},"Children":[{"ID":"20230817161882-zdcvupq","Type":"NodeListItem","Data":"1","ListData":{"Typ":1,"Tight":true,"Start":1,"Delimiter":46,"Padding":3,"Marker":"MQ==","Num":1},"Properties":{"id":"20230817161882-zdcvupq","updated":"20230817161882"},"Children":[{"ID":"20230817161883-a6tx0qp","Type":"NodeParagraph","Properties":{"id":"20230817161883-a6tx0qp","updated":"20230817161883"},"Children":[{"Type":"NodeText","Data":"MemoryStateBackend"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"State数据保存在Java堆内存中，执行Checkpoint的时候，会把State的快照数据保存到JobManager的内存中，基于内存的State Backend在生产环境下不建议使用。"}]}]},{"ID":"20230817161884-dmvaaf4","Type":"NodeListItem","Data":"2","ListData":{"Typ":1,"Tight":true,"Start":2,"Delimiter":46,"Padding":3,"Marker":"Mg==","Num":2},"Properties":{"id":"20230817161884-dmvaaf4","updated":"20230817161884"},"Children":[{"ID":"20230817161885-gghbe2g","Type":"NodeParagraph","Properties":{"id":"20230817161885-gghbe2g","updated":"20230817161885"},"Children":[{"Type":"NodeText","Data":"FsStateBackend"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"State数据保存在TaskManager的内存中，执行Checkpoint的时候，会把State的快照数据保存到配置的文件系统中，可以使用HDFS等分布式文件系统。"}]}]},{"ID":"20230817161886-5iokky6","Type":"NodeListItem","Data":"3","ListData":{"Typ":1,"Tight":true,"Start":3,"Delimiter":46,"Padding":3,"Marker":"Mw==","Num":3},"Properties":{"id":"20230817161886-5iokky6","updated":"20230817161886"},"Children":[{"ID":"20230817161887-k8b84p9","Type":"NodeParagraph","Properties":{"id":"20230817161887-k8b84p9","updated":"20230817161887"},"Children":[{"Type":"NodeText","Data":"RocksDBStateBackend"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"RocksDB跟上面的都略有不同，它会在本地文件系统中维护State，State会直接写入本地RocksDB中。同时它需要配置一个远端的文件系统（一般是HDFS），在做Checkpoint的时候，会把本地的数据直接复制到远端的文件系统中。故障切换的时候直接从远端的文件系统中恢复数据到本地。RocksDB克服了State受内存限制的缺点，同时又能够持久化到远端文件系统中，推荐在生产环境中使用。"}]}]}]},{"ID":"20230817161888-l4nw3bx","Type":"NodeParagraph","Properties":{"id":"20230817161888-l4nw3bx","updated":"20230817161888"},"Children":[{"Type":"NodeText","Data":"所以在这里我们使用第三种：RocksDBStateBackend"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"针对RocksDBStateBackend需要引入依赖"}]},{"ID":"20230817161889-segqlot","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161889-segqlot","updated":"20230817161889"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"\u003cdependency\u003e\n    \u003cgroupId\u003eorg.apache.flink\u003c/groupId\u003e\n    \u003cartifactId\u003eflink-statebackend-rocksdb_2.12\u003c/artifactId\u003e\n    \u003cversion\u003e1.11.1\u003c/version\u003e\n\u003c/dependency\u003e\n代码块12345\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161890-o3s89fe","Type":"NodeParagraph","Properties":{"id":"20230817161890-o3s89fe","updated":"20230817161890"},"Children":[{"Type":"NodeText","Data":"//设置状态数据存储的位置"}]},{"ID":"20230817161891-ln746ou","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161891-ln746ou","updated":"20230817161891"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"env.setStateBackend(new RocksDBStateBackend(\"hdfs://bigdata01:9000/flink/checkpoints\",true))\n代码块1\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161892-sy2xuim","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20230817161892-sy2xuim","updated":"20230817161892"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"## ","Properties":{"id":""}},{"Type":"NodeText","Data":"Kafka Consumers Offset 自动提交"}]},{"ID":"20230817161893-phn8j88","Type":"NodeParagraph","Properties":{"id":"20230817161893-phn8j88","updated":"20230817161893"},"Children":[{"Type":"NodeText","Data":"Kafka Consumers Offset自动提交机制需要根据Job是否开启Checkpoint来区分。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"CheckPoint关闭时：通过参数"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"enable.auto.commit"},{"Type":"NodeText","Data":"和"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"auto.commit.interval.ms"},{"Type":"NodeText","Data":"控制"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"CheckPoint开启时：执行CheckPoint的时候才会提交offset，此时kafka中的自动提交机制就会被忽略"}]},{"ID":"20230817161894-opyswqb","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20230817161894-opyswqb","updated":"20230817161894"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"## ","Properties":{"id":""}},{"Type":"NodeText","Data":"Kafka Producer的使用"}]},{"ID":"20230817161895-676jwrh","Type":"NodeParagraph","Properties":{"id":"20230817161895-676jwrh","updated":"20230817161895"},"Children":[{"Type":"NodeText","Data":"下面我们来看一下在Flink中如何向Kafka中写数据，此时需要用到Kafka Producer"}]},{"ID":"20230817161896-mi8al09","Type":"NodeParagraph","Properties":{"id":"20230817161896-mi8al09","updated":"20230817161896"},"Children":[{"Type":"NodeText","Data":"scala代码如下："}]},{"ID":"20230817161897-b4e57ep","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161897-b4e57ep","updated":"20230817161897"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"package com.imooc.scala.kafkaconnector\n\nimport java.util.Properties\n\nimport org.apache.flink.api.common.serialization.SimpleStringSchema\nimport org.apache.flink.streaming.api.scala.StreamExecutionEnvironment\nimport org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer\nimport org.apache.flink.streaming.connectors.kafka.internals.KafkaSerializationSchemaWrapper\nimport org.apache.flink.streaming.connectors.kafka.partitioner.FlinkFixedPartitioner\n\n/**\n * Flink向Kafka中生产数据\n * Created by xuwei\n */\nobject StreamKafkaSinkScala{\n  def main(args: Array[String]): Unit = {\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n\n    val text = env.socketTextStream(\"bigdata04\", 9001)\n\n    //指定FlinkKafkaProuducer相关配置\n    val topic = \"t2\"\n    val prop = new Properties()\n    prop.setProperty(\"bootstrap.servers\",\"bigdata01:9092,bigdata02:9092,bigdata03:9092\")\n\n    //指定kafka作为sink\n    /*\n    KafkaSerializationSchemaWrapper的几个参数\n    1：topic，指定需要写入的topic名称即可\n    2：partitioner，通过自定义分区器实现将数据写入到指定topic的具体分区中，\n    默认会使用FlinkFixedPartitioner，它表示会将所有的数据都写入指定topic的一个分区里面\n    如果不想自定义分区器，也不想使用默认的，可以直接使用null即可\n    3：writeTimeStamp，向topic中写入数据的时候，是否写入时间戳，\n    如果写入了，那么在watermark的案例中，使用extractTimestamp()提取时间戳的时候，\n    就可以直接使用previousElementTimestamp即可，它表示的就是我们在这里写入的数据对应的timestamp\n     */\n    val kafkaProducer = new FlinkKafkaProducer[String](topic, new KafkaSerializationSchemaWrapper[String](topic, new FlinkFixedPartitioner[String](),false, new SimpleStringSchema()), prop, FlinkKafkaProducer.Semantic.EXACTLY_ONCE)\n    text.addSink(kafkaProducer)\n\n    env.execute(\"StreamKafkaSinkScala\")\n  }\n}\n代码块123456789101112131415161718192021222324252627282930313233343536373839404142\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161898-t2wh3q5","Type":"NodeParagraph","Properties":{"id":"20230817161898-t2wh3q5","updated":"20230817161898"},"Children":[{"Type":"NodeText","Data":"在执行代码之前，需要创建topic：t2"}]},{"ID":"20230817161899-4h1j71z","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161899-4h1j71z","updated":"20230817161899"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"[root@bigdata01 kafka_2.12-2.4.1]# bin/kafka-topics.sh --create --zookeeper localhost:2181 --partitions 5 --replication-factor 2 --topic t2\n代码块1\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161900-gh1k7hi","Type":"NodeParagraph","Properties":{"id":"20230817161900-gh1k7hi","updated":"20230817161900"},"Children":[{"Type":"NodeText","Data":"开启socket，产生数据"}]},{"ID":"20230817161901-qsoubug","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161901-qsoubug","updated":"20230817161901"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"[root@bigdata04 ~]# nc -l 9001\nhello   \nhaha\nabc\nxyz\nhga\nljk\nhui\nopj\n代码块123456789\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161902-vzqbiu2","Type":"NodeParagraph","Properties":{"id":"20230817161902-vzqbiu2","updated":"20230817161902"},"Children":[{"Type":"NodeText","Data":"最终查看t2中数据的分布，发现所有数据都在一个分区中"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeImage","Properties":{"id":""},"Children":[{"Type":"NodeBang","Data":"!","Properties":{"id":""}},{"Type":"NodeOpenBracket","Data":"[","Properties":{"id":""}},{"Type":"NodeLinkText","Data":"图片描述","Properties":{"id":""}},{"Type":"NodeCloseBracket","Data":"]","Properties":{"id":""}},{"Type":"NodeOpenParen","Data":"(","Properties":{"id":""}},{"Type":"NodeLinkDest","Data":"https://img.mukewang.com/wiki/5f86bdb409927be417260457.jpg","Properties":{"id":""}},{"Type":"NodeCloseParen","Data":")","Properties":{"id":""}}]}]},{"ID":"20230817161903-etnt4gw","Type":"NodeParagraph","Properties":{"id":"20230817161903-etnt4gw","updated":"20230817161903"},"Children":[{"Type":"NodeText","Data":"所以，如果我们不需要自定义分区器的时候，直接传递为null即可，不要使用"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"FlinkFixedPartitioner"},{"Type":"NodeText","Data":"，它会将数据都写入到topic的一个分区中。"}]},{"ID":"20230817161904-qz2ccl7","Type":"NodeParagraph","Properties":{"id":"20230817161904-qz2ccl7","updated":"20230817161904"},"Children":[{"Type":"NodeText","Data":"将FlinkFixedPartitioner设置为null，重新验证一次，创建一个新的topic，t3"}]},{"ID":"20230817161905-2mj661q","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161905-2mj661q","updated":"20230817161905"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"[root@bigdata01 kafka_2.12-2.4.1]# bin/kafka-topics.sh --create --zookeeper localhost:2181 --partitions 5 --replication-factor 2 --topic t3\n代码块1\nval topic = \"t3\"\n....\nval kafkaProducer = new FlinkKafkaProducer[String](topic, new KafkaSerializationSchemaWrapper[String](topic, null,false, new SimpleStringSchema()), prop, FlinkKafkaProducer.Semantic.EXACTLY_ONCE)\n代码块123\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161906-q9ugd76","Type":"NodeParagraph","Properties":{"id":"20230817161906-q9ugd76","updated":"20230817161906"},"Children":[{"Type":"NodeText","Data":"重新启动程序，验证效果"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"开启socket，产生数据"}]},{"ID":"20230817161907-yelp50r","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161907-yelp50r","updated":"20230817161907"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"[root@bigdata04 ~]# nc -l 9001\nhello   \nhaha\nabc\nxyz\nhga\nljk\nhui\nopj\n代码块123456789\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161908-wvgsytd","Type":"NodeParagraph","Properties":{"id":"20230817161908-wvgsytd","updated":"20230817161908"},"Children":[{"Type":"NodeText","Data":"查看结果"}]},{"ID":"20230817161909-3exd9fe","Type":"NodeParagraph","Properties":{"id":"20230817161909-3exd9fe","updated":"20230817161909"},"Children":[{"Type":"NodeImage","Properties":{"id":""},"Children":[{"Type":"NodeBang","Data":"!","Properties":{"id":""}},{"Type":"NodeOpenBracket","Data":"[","Properties":{"id":""}},{"Type":"NodeLinkText","Data":"图片描述","Properties":{"id":""}},{"Type":"NodeCloseBracket","Data":"]","Properties":{"id":""}},{"Type":"NodeOpenParen","Data":"(","Properties":{"id":""}},{"Type":"NodeLinkDest","Data":"https://img.mukewang.com/wiki/5f86bddd09efd22618250455.jpg","Properties":{"id":""}},{"Type":"NodeCloseParen","Data":")","Properties":{"id":""}}]},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"java代码如下："}]},{"ID":"20230817161910-9kn890f","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161910-9kn890f","updated":"20230817161910"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"package com.imooc.java.kafkaconnector;\n\nimport org.apache.flink.api.common.serialization.SimpleStringSchema;\nimport org.apache.flink.streaming.api.datastream.DataStreamSource;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\nimport org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer;\nimport org.apache.flink.streaming.connectors.kafka.internals.KafkaSerializationSchemaWrapper;\n\nimport java.util.Properties;\n\n/**\n * Flink向Kafka中生产数据\n * Created by xuwei\n */\npublic class StreamKafkaSinkJava {\n    public static void main(String[] args) throws Exception{\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n        DataStreamSource\u003cString\u003e text = env.socketTextStream(\"bigdata04\", 9001);\n\n        //指定FlinkKafkaProducer相关配置\n        String topic = \"t3\";\n        Properties prop = new Properties();\n        prop.setProperty(\"bootstrap.servers\",\"bigdata01:9092,bigdata02:9092,bigdata03:9092\");\n\n        //指定kafka作为sink\n        FlinkKafkaProducer\u003cString\u003e kafkaProducer = new FlinkKafkaProducer\u003c\u003e(topic, new KafkaSerializationSchemaWrapper\u003cString\u003e(topic, null, false, new SimpleStringSchema()), prop, FlinkKafkaProducer.Semantic.EXACTLY_ONCE);\n        text.addSink(kafkaProducer);\n\n        env.execute(\"StreamKafkaSinkJava\");\n    }\n}\n代码块1234567891011121314151617181920212223242526272829303132\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161911-a31qq87","Type":"NodeHeading","HeadingLevel":2,"Properties":{"id":"20230817161911-a31qq87","updated":"20230817161911"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"## ","Properties":{"id":""}},{"Type":"NodeText","Data":"Kafka Producer的容错"}]},{"ID":"20230817161912-dggk8si","Type":"NodeParagraph","Properties":{"id":"20230817161912-dggk8si","updated":"20230817161912"},"Children":[{"Type":"NodeText","Data":"如果Flink开启了CheckPoint，针对FlinkKafkaProducer可以提供EXACTLY_ONCE的语义保证"}]},{"ID":"20230817161913-ypq6gj7","Type":"NodeParagraph","Properties":{"id":"20230817161913-ypq6gj7","updated":"20230817161913"},"Children":[{"Type":"NodeText","Data":"可以通过semantic 参数来选择三种不同的语义："}]},{"ID":"20230817161914-dx1ln2t","Type":"NodeParagraph","Properties":{"id":"20230817161914-dx1ln2t","updated":"20230817161914"},"Children":[{"Type":"NodeText","Data":"Semantic.NONE、Semantic.AT_LEAST_ONCE【默认】、Semantic.EXACTLY_ONCE"}]},{"ID":"20230817161915-z3ocalu","Type":"NodeParagraph","Properties":{"id":"20230817161915-z3ocalu","updated":"20230817161915"},"Children":[{"Type":"NodeText","Data":"scala代码如下："}]},{"ID":"20230817161916-wzsrhwl","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161916-wzsrhwl","updated":"20230817161916"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"package com.imooc.scala.kafkaconnector\n\nimport java.util.Properties\n\nimport org.apache.flink.api.common.serialization.SimpleStringSchema\nimport org.apache.flink.streaming.api.scala.StreamExecutionEnvironment\nimport org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer\nimport org.apache.flink.streaming.connectors.kafka.internals.KafkaSerializationSchemaWrapper\nimport org.apache.flink.streaming.connectors.kafka.partitioner.FlinkFixedPartitioner\n\n/**\n * Flink向Kafka中生产数据\n * Created by xuwei\n */\nobject StreamKafkaSinkScala{\n  def main(args: Array[String]): Unit = {\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n    //开启checkpoint\n    env.enableCheckpointing(5000)\n\n    val text = env.socketTextStream(\"bigdata04\", 9001)\n\n    //指定FlinkKafkaProducer相关配置\n    val topic = \"t3\"\n    val prop = new Properties()\n    prop.setProperty(\"bootstrap.servers\",\"bigdata01:9092,bigdata02:9092,bigdata03:9092\")\n\n    //指定kafka作为sink\n    /*\n    KafkaSerializationSchemaWrapper的几个参数\n    1：topic，指定需要写入的topic名称即可\n    2：partitioner，通过自定义分区器实现将数据写入到指定topic的具体分区中，\n    默认会使用FlinkFixedPartitioner，它表示会将所有的数据都写入指定topic的一个分区里面\n    如果不想自定义分区器，也不想使用默认的，可以直接使用null即可\n    3：writeTimeStamp，向topic中写入数据的时候，是否写入时间戳，\n    如果写入了，那么在watermark的常见中，使用extractTimestamp()提取时间戳的时候，\n    就可以直接使用previousElementTimestamp即可，它表示的就是我们在这里写入的数据对应的timestamp\n    4：SerializationSchema，数据解析规则，默认使用string类型的数据解析规则即可\n     */\n    val kafkaProducer = new FlinkKafkaProducer[String](topic, new KafkaSerializationSchemaWrapper[String](topic, null,false, new SimpleStringSchema()), prop, FlinkKafkaProducer.Semantic.EXACTLY_ONCE)\n    text.addSink(kafkaProducer)\n\n    env.execute(\"StreamKafkaSinkScala\")\n  }\n}\n代码块123456789101112131415161718192021222324252627282930313233343536373839404142434445\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161917-rp5w2w7","Type":"NodeBlockquote","Properties":{"id":"20230817161917-rp5w2w7","updated":"20230817161917"},"Children":[{"Type":"NodeBlockquoteMarker","Data":"\u003e ","Properties":{"id":""}},{"ID":"20230817161918-5vm1oq0","Type":"NodeParagraph","Properties":{"id":"20230817161918-5vm1oq0","updated":"20230817161918"},"Children":[{"Type":"NodeText","Data":"注意：此时执行代码会发现无法正常执行，socket打开之后，启动代码，会发现socket监听会自动断开，表示代码执行断开了。"}]}]},{"ID":"20230817161919-22etuwq","Type":"NodeParagraph","Properties":{"id":"20230817161919-22etuwq","updated":"20230817161919"},"Children":[{"Type":"NodeText","Data":"但是此时在idea中看不到任何报错信息，主要是因为我们之前把日志级别改为error级别了，把日志级别调整为warn之后就可以看到报错信息了"}]},{"ID":"20230817161920-029ogxo","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161920-029ogxo","updated":"20230817161920"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"log4j.rootLogger=warn,stdout\n\nlog4j.appender.stdout = org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.Target = System.out\nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout \nlog4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} [%t] [%c] [%p] - %m%n\n代码块123456\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161921-asjwouk","Type":"NodeParagraph","Properties":{"id":"20230817161921-asjwouk","updated":"20230817161921"},"Children":[{"Type":"NodeText","Data":"报错信息如下："}]},{"ID":"20230817161922-yf7vcpd","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161922-yf7vcpd","updated":"20230817161922"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"2020-08-12 19:21:59,759 [Sink: Unnamed (3/8)] [org.apache.flink.runtime.taskmanager.Task] [WARN] - Sink: Unnamed (3/8) (1b621d88e460877995ad37d34379c166) switched from RUNNING to FAILED.\norg.apache.kafka.common.KafkaException: Unexpected error in InitProducerIdResponse; The transaction timeout is larger than the maximum value allowed by the broker (as configured by transaction.max.timeout.ms).\n\tat org.apache.kafka.clients.producer.internals.TransactionManager$InitProducerIdHandler.handleResponse(TransactionManager.java:1151)\n\tat org.apache.kafka.clients.producer.internals.TransactionManager$TxnRequestHandler.onComplete(TransactionManager.java:1074)\n\tat org.apache.kafka.clients.ClientResponse.onComplete(ClientResponse.java:109)\n\tat org.apache.kafka.clients.NetworkClient.completeResponses(NetworkClient.java:569)\n\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:561)\n\tat org.apache.kafka.clients.producer.internals.Sender.maybeSendAndPollTransactionalRequest(Sender.java:425)\n\tat org.apache.kafka.clients.producer.internals.Sender.runOnce(Sender.java:311)\n\tat org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:244)\n\tat java.lang.Thread.run(Thread.java:748)\n代码块1234567891011\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161923-0k48weg","Type":"NodeParagraph","Properties":{"id":"20230817161923-0k48weg","updated":"20230817161923"},"Children":[{"Type":"NodeText","Data":"提示生产者中设置的事务超时时间大于broker中设置的事务超时时间。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"因为Kafka服务中默认事务的超时时间是15min，但是FlinkKafkaProducer里面设置的事务超时时间默认是1h。EXACTLY_ONCE 模式依赖于事务，如果从 Flink 应用程序崩溃到完全重启的时间超过了 Kafka 的事务超时时间，那么将会有数据丢失，所以我们需要合理地配置事务超时时间，因此在使用 EXACTLY_ONCE 模式之前建议增加 Kafka broker 中"},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"code","TextMarkTextContent":"transaction.max.timeout.ms"},{"Type":"NodeText","Data":" 的值。"}]},{"ID":"20230817161924-j8mpayi","Type":"NodeParagraph","Properties":{"id":"20230817161924-j8mpayi","updated":"20230817161924"},"Children":[{"Type":"NodeText","Data":"下面我们需要修改kafka中的server.properties配置文件"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"bigdata01、bigdata02、bigdata03都需要修改"}]},{"ID":"20230817161925-58ny03f","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161925-58ny03f","updated":"20230817161925"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"[root@bigdata01 kafka_2.12-2.4.1]# vi config/server.properties\n...\ntransaction.max.timeout.ms=3600000\n[root@bigdata02 kafka_2.12-2.4.1]# vi config/server.properties\n...\ntransaction.max.timeout.ms=3600000\n[root@bigdata03 kafka_2.12-2.4.1]# vi config/server.properties\n...\ntransaction.max.timeout.ms=3600000\n代码块123456789\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161926-u1pj71s","Type":"NodeParagraph","Properties":{"id":"20230817161926-u1pj71s","updated":"20230817161926"},"Children":[{"Type":"NodeText","Data":"改完配置文件之后，重启kafka"}]},{"ID":"20230817161927-5rjnglw","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161927-5rjnglw","updated":"20230817161927"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"[root@bigdata01 kafka_2.12-2.4.1]# JMX_PORT=9988 bin/kafka-server-start.sh -daemon config/server.properties \n[root@bigdata02 kafka_2.12-2.4.1]# JMX_PORT=9988 bin/kafka-server-start.sh -daemon config/server.properties \n[root@bigdata03 kafka_2.12-2.4.1]# JMX_PORT=9988 bin/kafka-server-start.sh -daemon config/server.properties \n代码块123\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161928-2ovwb89","Type":"NodeParagraph","Properties":{"id":"20230817161928-2ovwb89","updated":"20230817161928"},"Children":[{"Type":"NodeText","Data":"重新执行Flink代码，此时就不报错了。"}]}]}