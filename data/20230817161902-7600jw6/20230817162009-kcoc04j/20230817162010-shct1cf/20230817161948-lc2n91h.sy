{"ID":"20230817161948-lc2n91h","Spec":"1","Type":"NodeDocument","Properties":{"id":"20230817161948-lc2n91h","title":"Sparksql","updated":"20230817161948"},"Children":[{"ID":"20230817161949-skyu4b7","Type":"NodeHeading","HeadingLevel":3,"Properties":{"id":"20230817161949-skyu4b7","updated":"20230817161949"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"### ","Properties":{"id":""}},{"Type":"NodeText","Data":"Sparksql"}]},{"ID":"20230817161950-qtzujc6","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20230817161950-qtzujc6","updated":"20230817161950"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"##### ","Properties":{"id":""}},{"Type":"NodeText","Data":"dataframe的一些算子"}]},{"ID":"20230817161951-mkqo74o","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockInfo":"amF2YQ==","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161951-mkqo74o","updated":"20230817161951"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","CodeBlockInfo":"amF2YQ==","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"package com.pyy.spark.sql\n\nimport org.apache.spark\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\n\n/**\n * 用sparksql操作json文件\n */\nobject SparkSqlUse {\n  def main(args: Array[String]): Unit = {\n     val conf =new SparkConf().setMaster(\"local\")\n    //创建sparksession对象\n    val sparksession = SparkSession.builder().\n      appName(\"SparkSqlUse\").config(conf).getOrCreate()\n     //读取json文件\n     val dataframe = sparksession.read.json(\"E:\\\\student.json\")\n     //显示dataframe的信息\n    dataframe.show(2)\n    //显示表信息\n    dataframe.printSchema()\n    //获取name这一列的数据\n    dataframe.select(\"name\").show()\n    import sparksession.implicits._\n    dataframe.select($\"name\",$\"age\"+1).show()//如想要对年龄加一 需要导入隐式转换函数\n    //过滤也需要加隐式转换函数\n    dataframe.filter($\"age\"\u003e18).show()\n    dataframe.where($\"age\"\u003e18).show()\n    dataframe.groupBy(\"sex\").count().show()\n    sparksession.stop()\n  }\n}\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161952-tuz70wp","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20230817161952-tuz70wp","updated":"20230817161952"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"##### ","Properties":{"id":""}},{"Type":"NodeText","Data":"spark还可以将dataframe这个逻辑概念转成一张表 然后我们已通过sql语句进行对这张表的操作"}]},{"ID":"20230817161953-c7bk3kf","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockInfo":"amF2YQ==","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161953-c7bk3kf","updated":"20230817161953"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","CodeBlockInfo":"amF2YQ==","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"import org.apache.spark.sql.SparkSession\n\n/**\n * 使用sql操作frame\n */\nobject DataFrameSqlScala {\n  def main(args: Array[String]): Unit = {\n    val conf =new SparkConf().setMaster(\"local\")\n    //创建sparksession对象\n    val sparksession = SparkSession.builder().\n      appName(\"SparkSqlUse\").config(conf).getOrCreate()\n    //读取json文件\n    val dataframe = sparksession.read.json(\"E:\\\\student.json\")\n    dataframe.createOrReplaceTempView(\"student\")\n    sparksession.sql(\"select age,count(*)as num from student group by age\").show()\n    sparksession.stop()\n  }\n}\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161954-q4ft51v","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20230817161954-q4ft51v","updated":"20230817161954"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"##### ","Properties":{"id":""}},{"Type":"NodeText","Data":"将RDD转换成sparksession"}]},{"ID":"20230817161955-x8huhko","Type":"NodeParagraph","Properties":{"id":"20230817161955-x8huhko","updated":"20230817161955"},"Children":[{"Type":"NodeText","Data":"1、通过反射方式 该方式前提是知道rdd的结构信息 可以通过case class 类隐式转换成dataframe"}]},{"ID":"20230817161956-nbplwz9","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockInfo":"c2NhbGE=","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161956-nbplwz9","updated":"20230817161956"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","CodeBlockInfo":"c2NhbGE=","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"package com.pyy.spark.sql\n\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\n\n/**\n * 使用反射方式来讲RDD构建成dataframe\n * 前提是知道rdd里面的数据格式 知道了以后才能在case calss中创建字段\n */\nobject RddtoDataFrame {\n  def main(args: Array[String]): Unit = {\n    val conf =new SparkConf().setMaster(\"local\")\n    //创建sparksession对象\n    val sparksession = SparkSession.builder().\n      appName(\"SparkSqlUse\").config(conf).getOrCreate()\n    val sparkcontext = sparksession.sparkContext\n     val dataRDD=sparkcontext.parallelize(Array((\"pyy\",18),(\"jack\",19)))\n   //引入隐式转换函数\n    import sparksession.implicits._\n    //通过该样例类会将样例类的字段作为列名 最后转换成dataframe\n    val sduDf=dataRDD.map(tup=\u003eStudent(tup._1,tup._2)).toDF()\n    //类似于创建一个表 来使用sql语句进行查询\n    val view = sduDf.createOrReplaceTempView(\"student\")\n    val resDf = sparksession.sql(\"select name,age from student where age\u003e18\").show()\n\n    //再讲resdf转换成rdd\n    val resrdd=sduDf.rdd\n    resrdd.map(row=\u003eStudent(row.getAs[String](\"name\"),row.getAs[Int](\"age\")))\n        .collect().foreach(println(_))\n    sparkcontext.stop()\n    sparksession.stop()\n  }\n}\ncase  class  Student(name :String,age:Int)\n\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161957-oktiznf","Type":"NodeParagraph","Properties":{"id":"20230817161957-oktiznf","updated":"20230817161957"},"Children":[{"Type":"NodeText","Data":"2、通过程序实现"}]},{"ID":"20230817161958-uhguphk","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockInfo":"c2NhbGE=","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161958-uhguphk","updated":"20230817161958"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","CodeBlockInfo":"c2NhbGE=","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"package com.pyy.spark.sql\n\n/**\n * 通过程序组装dataframe\n */\n\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\nimport org.apache.spark.sql.{Row, SparkSession}\n\nobject RDDtoDataFramebyProgram {\n  def main(args: Array[String]): Unit = {\n    val conf =new SparkConf().setMaster(\"local\")\n    //创建sparksession对象\n    val sparksession = SparkSession.builder().\n      appName(\"RDDtoDataFramebyProgram\").config(conf).getOrCreate()\n    val sparkcontext = sparksession.sparkContext\n    val dataRDD=sparkcontext.parallelize(Array((\"pyy\",18),(\"jack\",19)))\n    //组装rowRDD\n    val rowRDD=dataRDD.map(tup=\u003eRow(tup._1,tup._2));\n    //指定元数据信息 也就是表结构 这个信息可以从外部获取 比较灵活\n    val schema=StructType(Array(\n      StructField(\"name\",StringType,true),\n      StructField(\"age\",IntegerType,true)\n    ))\n    //组装dataframe\n    val studf=sparksession.createDataFrame(rowRDD,schema)\n    studf.printSchema()\n    studf.createOrReplaceTempView(\"student\");\n    val resdf=sparksession.sql(\"select * from student where age\u003e18\")\n\n//    val resRDD = resdf.rdd\n//    resRDD.map(row=\u003e(row(0).toString,row(1).toString.toInt)).foreach(println(_))\n    sparksession.stop()\n  }\n}\n\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161959-v2ywztd","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20230817161959-v2ywztd","updated":"20230817161959"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"##### ","Properties":{"id":""}},{"Type":"NodeText","Data":"load和save的用法"}]},{"ID":"20230817161960-j4cjo67","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockInfo":"amF2YQ==","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161960-j4cjo67","updated":"20230817161960"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","CodeBlockInfo":"amF2YQ==","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"package com.pyy.spark.sql\n\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\n\nobject LoadAndSaveOpScala {\n  def main(args: Array[String]): Unit = {\n    val conf =new SparkConf().setMaster(\"local\")\n    //创建sparksession对象\n    val sparksession = SparkSession.builder().\n      appName(\"LoadAndSaveOpScala \").config(conf).getOrCreate()\n    //读取json文件\n    val dataframe = sparksession.read.format(\"json\").load(\"E:\\\\student.json\")\n    dataframe.select(\"name\",\"age\").\n      write.format(\"csv\").save(\"hdfs://bigdata01:9000/out-save001\")\n    sparksession.stop()\n  }\n}\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161961-6wjfl17","Type":"NodeParagraph","Properties":{"id":"20230817161961-6wjfl17","updated":"20230817161961"},"Children":[{"Type":"NodeText","Data":"自定义UDF实现将首字母转换为大写"}]},{"ID":"20230817161962-x0rhbei","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockInfo":"amF2YQ==","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161962-x0rhbei","updated":"20230817161962"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","CodeBlockInfo":"amF2YQ==","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"package com.pyy.spark.sql\n\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\n\nobject SparkSqlUDF {\n  def main(args: Array[String]): Unit = {\n    val conf =new SparkConf().setMaster(\"local\")\n    //创建sparksession对象\n    val sparksession = SparkSession.builder().\n      appName(\"LoadAndSaveOpScala \").config(conf).getOrCreate()\n    //读取json文件\n    val dataframe = sparksession.read.format(\"json\").load(\"E:\\\\student.json\")\n    dataframe.createOrReplaceTempView(\"student\")\n    sparksession.udf.register(\"up_first\",(str:String)=\u003e{\n         val s1=str(0).toUpper\n         val  s2=str.substring(1)\n         val s3=s1+s2\n        s3\n    })\n    sparksession.sql(\"select up_first(name) as name,age from student where age\u003e18\").show()\n    sparksession.stop()\n  }\n}\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161963-m2as761","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20230817161963-m2as761","updated":"20230817161963"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"##### ","Properties":{"id":""}},{"Type":"NodeText","Data":"sparksql与hive的一些区别"}]},{"ID":"20230817161964-a7fzlyx","Type":"NodeParagraph","Properties":{"id":"20230817161964-a7fzlyx","updated":"20230817161964"},"Children":[{"Type":"NodeText","Data":"1：从架构层面对比分析有什么异同"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"Hive和SparkSql,都是一种sql解析器引擎，,但是SparkSql是基于spark这个分布式计算引擎的，而Hive可基于hadoop的mapreduce、spark这样一些不同的计算引擎 同时hive是一个存储加sql分析一体化的数据仓库"}]},{"ID":"20230817161965-5q0aqly","Type":"NodeParagraph","Properties":{"id":"20230817161965-5q0aqly","updated":"20230817161965"},"Children":[{"Type":"NodeText","Data":"2：从应用场景层面对比分析有什么异同"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"都可以用来在HDFS大规模数据集上很方便地利用SQL 语言查询、汇总、分析数据"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"Hive适合处理离线非实时数据"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"SparkSql适合实时性要求或者速度要求较高的场所"}]},{"ID":"20230817161966-ebu70yt","Type":"NodeParagraph","Properties":{"id":"20230817161966-ebu70yt","updated":"20230817161966"},"Children":[{"Type":"NodeText","Data":"所以企业中常用hive+sparksql来处理数据 hive可以基于hdfs存储数据"}]},{"ID":"20230817161967-dfb16nh","Type":"NodeParagraph","Properties":{"id":"20230817161967-dfb16nh","updated":"20230817161967"},"Children":[{"Type":"NodeText","Data":"hive的hql用于处理一些非实时的数据"}]},{"ID":"20230817161968-6sfwa9c","Type":"NodeParagraph","Properties":{"id":"20230817161968-6sfwa9c","updated":"20230817161968"},"Children":[{"Type":"NodeText","Data":"sparksql处理一下实时性强的数据"}]},{"ID":"20230817161969-vguing8","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20230817161969-vguing8","updated":"20230817161969"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"##### ","Properties":{"id":""}},{"Type":"NodeText","Data":"Sparksql的执行流程"}]}]}