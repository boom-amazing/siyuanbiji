{"ID":"20230817161956-nrasypq","Spec":"1","Type":"NodeDocument","Properties":{"id":"20230817161956-nrasypq","title":"InputFormat分析","updated":"20230817161956"},"Children":[{"ID":"20230817161957-u8i2o1t","Type":"NodeHeading","HeadingLevel":4,"Properties":{"id":"20230817161957-u8i2o1t","updated":"20230817161957"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"#### ","Properties":{"id":""}},{"Type":"NodeText","Data":"InputFormat分析"}]},{"ID":"20230817161958-qn83h8s","Type":"NodeParagraph","Properties":{"id":"20230817161958-qn83h8s","updated":"20230817161958"},"Children":[{"Type":"NodeText","Data":"Hadoop中有一个抽象类是InputFormat，InputFormat抽象类是MapReduce输入数据的顶层基类，这个抽象类中只定义了两个方法"}]},{"ID":"20230817161959-id3qhyv","Type":"NodeList","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161959-id3qhyv","updated":"20230817161959"},"Children":[{"ID":"20230817161960-qpuweo9","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161960-qpuweo9","updated":"20230817161960"},"Children":[{"ID":"20230817161961-g5wlgs0","Type":"NodeParagraph","Properties":{"id":"20230817161961-g5wlgs0","updated":"20230817161961"},"Children":[{"Type":"NodeText","Data":"一个是getSplits方法"}]}]},{"ID":"20230817161962-4duiq7g","Type":"NodeListItem","Data":"-","ListData":{"Tight":true,"BulletChar":45,"Padding":2,"Marker":"LQ==","Num":-1},"Properties":{"id":"20230817161962-4duiq7g","updated":"20230817161962"},"Children":[{"ID":"20230817161963-txdgx4x","Type":"NodeParagraph","Properties":{"id":"20230817161963-txdgx4x","updated":"20230817161963"},"Children":[{"Type":"NodeText","Data":"另一个是createRecordReader方法"}]}]}]},{"ID":"20230817161964-8wwfgmt","Type":"NodeParagraph","Properties":{"id":"20230817161964-8wwfgmt","updated":"20230817161964"},"Children":[{"Type":"NodeText","Data":"getsplits方法是对文件进行逻辑切分"}]},{"ID":"20230817161965-h12h07w","Type":"NodeParagraph","Properties":{"id":"20230817161965-h12h07w","updated":"20230817161965"},"Children":[{"Type":"NodeText","Data":"源代码："}]},{"ID":"20230817161966-eygrsmd","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockInfo":"amF2YQ==","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161966-eygrsmd","updated":"20230817161966"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","CodeBlockInfo":"amF2YQ==","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"public List\u003cInputSplit\u003e getSplits(JobContext job) throws IOException {\n  StopWatch sw = new StopWatch().start();\n  //获取InputSplit的size的最小值minSize和最大值maxSize\n  /*\n  getFormatMinSplitSize()=1\n  getMinSplitSize(job)=0\n  所以最终minSize=1\n   */\n  long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));\n  /*\n  getMaxSplitSize(job)=Long.MAX_VALUE\n  所以maxSize等于Long的最大值\n   */\n  long maxSize = getMaxSplitSize(job);\n\n  // 创建List，准备保存生成的InputSplit\n  List\u003cInputSplit\u003e splits = new ArrayList\u003cInputSplit\u003e();\n  //获取输入文件列表\n  List\u003cFileStatus\u003e files = listStatus(job);\n\n  /*\n  !getInputDirRecursive(job) = !false = true\n  job.getConfiguration().getBoolean(INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS, false) = false\n  所以ignoreDirs=false\n   */\n  boolean ignoreDirs = !getInputDirRecursive(job)\n    \u0026\u0026 job.getConfiguration().getBoolean(INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS, false);\n  //迭代输入文件列表\n  for (FileStatus file: files) {\n    //是否忽略子目录，默认不忽略\n    if (ignoreDirs \u0026\u0026 file.isDirectory()) {\n      continue;\n    }\n    //获取 文件/目录 路径\n    Path path = file.getPath();\n    //获取 文件/目录 长度\n    long length = file.getLen();\n    if (length != 0) {\n      //保存文件的Block块所在的位置\n      BlockLocation[] blkLocations;\n      if (file instanceof LocatedFileStatus) {\n        blkLocations = ((LocatedFileStatus) file).getBlockLocations();\n      } else {\n        FileSystem fs = path.getFileSystem(job.getConfiguration());\n        blkLocations = fs.getFileBlockLocations(file, 0, length);\n      }\n      //判断文件是否支持切割，默认为true\n      if (isSplitable(job, path)) {\n        //获取文件的Block大小，默认128M\n        long blockSize = file.getBlockSize();\n        //计算split的大小\n        /*\n        内部使用的公式是：Math.max(minSize, Math.min(maxSize, blockSize))\n        splitSize = Math.max(1, Math.min(Long.MAX_VALUE, 128))=128M=134217728字节\n        所以我们说默认情况下split逻辑切片的大小和Block size相等\n         */\n        long splitSize = computeSplitSize(blockSize, minSize, maxSize);\n\n        //还需要处理的文件剩余字节大小，其实就是这个文件的原始大小\n        long bytesRemaining = length;\n        //\n        /*\n        SPLIT_SLOP = 1.1\n        文件剩余字节大小/1134217728【128M】 \u003e 1.1\n        意思就是当文件剩余大小bytesRemaining与splitSize的比值还大于1.1的时候，就继续切分，\n        否则，剩下的直接作为一个InputSplit\n        敲黑板，划重点：只要bytesRemaining/splitSize\u003c=1.1就会停止划分，将剩下的作为一个InputSplit\n         */\n        while (((double) bytesRemaining)/splitSize \u003e SPLIT_SLOP) {\n          int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);\n          //组装InputSplit\n          /*\n          生成InputSplit\n          path：路径\n          length-bytesRemaining：起始位置\n          splitSize：大小\n          blkLocations[blkIndex].getHosts()和blkLocations[blkIndex].getCachedHosts()：所在的 host（节点） 列表\n          makeSplit(path, length-bytesRemaining, splitSize,\n                      blkLocations[blkIndex].getHosts(),\n                      blkLocations[blkIndex].getCachedHosts())\n           */\n          splits.add(makeSplit(path, length-bytesRemaining, splitSize,\n                      blkLocations[blkIndex].getHosts(),\n                      blkLocations[blkIndex].getCachedHosts()));\n          bytesRemaining -= splitSize;\n        }\n\n        //最后会把bytesRemaining/splitSize\u003c=1.1的那一部分内容作为一个InputSplit\n        if (bytesRemaining != 0) {\n          int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);\n          splits.add(makeSplit(path, length-bytesRemaining, bytesRemaining,\n                     blkLocations[blkIndex].getHosts(),\n                     blkLocations[blkIndex].getCachedHosts()));\n        }\n      } else { // not splitable\n        //如果文件不支持切割，执行这里\n        if (LOG.isDebugEnabled()) {\n          // Log only if the file is big enough to be splitted\n          if (length \u003e Math.min(file.getBlockSize(), minSize)) {\n            LOG.debug(\"File is not splittable so no parallelization \"\n                + \"is possible: \" + file.getPath());\n          }\n        }\n        //把不支持切割的文件整个作为一个InputSplit\n        splits.add(makeSplit(path, 0, length, blkLocations[0].getHosts(),\n                    blkLocations[0].getCachedHosts()));\n      }\n    } else { \n      //Create empty hosts array for zero length files\n      splits.add(makeSplit(path, 0, length, new String[0]));\n    }\n  }\n  // Save the number of input files for metrics/loadgen\n  job.getConfiguration().setLong(NUM_INPUT_FILES, files.size());\n  sw.stop();\n  if (LOG.isDebugEnabled()) {\n    LOG.debug(\"Total # of splits generated by getSplits: \" + splits.size()\n        + \", TimeTaken: \" + sw.now(TimeUnit.MILLISECONDS));\n  }\n  return splits;\n}\n\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]},{"ID":"20230817161967-oxqhz2g","Type":"NodeParagraph","Properties":{"id":"20230817161967-oxqhz2g","updated":"20230817161967"},"Children":[{"Type":"NodeText","Data":"createRecordReader是创建一个行阅读器 对切分出来的inputsplit进行读取"}]},{"ID":"20230817161968-o3lma0n","Type":"NodeParagraph","Properties":{"id":"20230817161968-o3lma0n","updated":"20230817161968"},"Children":[{"Type":"NodeText","Data":"源码："}]},{"ID":"20230817161969-3y1729q","Type":"NodeCodeBlock","IsFencedCodeBlock":true,"CodeBlockFenceChar":96,"CodeBlockFenceLen":3,"CodeBlockOpenFence":"YGBg","CodeBlockInfo":"amF2YQ==","CodeBlockCloseFence":"YGBg","Properties":{"id":"20230817161969-3y1729q","updated":"20230817161969"},"Children":[{"Type":"NodeCodeBlockFenceOpenMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}},{"Type":"NodeCodeBlockFenceInfoMarker","CodeBlockInfo":"amF2YQ==","Properties":{"id":""}},{"Type":"NodeCodeBlockCode","Data":"//初始化方法\npublic void initialize(InputSplit genericSplit,\n                       TaskAttemptContext context) throws IOException {\n  //获取传过来的InputSplit，将InputSplit转换成子类FileSplit\n  FileSplit split = (FileSplit) genericSplit;\n  Configuration job = context.getConfiguration();\n  //MAX_LINE_LENGTH对应的参数默认没有设置，所以会取Integer.MAX_VALUE\n  this.maxLineLength = job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n  //获取InputSplit的起始位置\n  start = split.getStart();\n  //获取InputSplit的结束位置\n  end = start + split.getLength();\n  //获取InputSplit的路径\n  final Path file = split.getPath();\n\n  // 打开文件，并且跳到InputSplit的起始位置\n  final FileSystem fs = file.getFileSystem(job);\n  fileIn = fs.open(file);\n\n  // 获取文件的压缩信息\n  CompressionCodec codec = new CompressionCodecFactory(job).getCodec(file);\n  //如果文件是压缩文件，则执行if中的语句\n  if (null!=codec) {\n    isCompressedInput = true;\n    decompressor = CodecPool.getDecompressor(codec);\n    if (codec instanceof SplittableCompressionCodec) {\n      final SplitCompressionInputStream cIn =\n        ((SplittableCompressionCodec)codec).createInputStream(\n          fileIn, decompressor, start, end,\n          SplittableCompressionCodec.READ_MODE.BYBLOCK);\n      in = new CompressedSplitLineReader(cIn, job,\n          this.recordDelimiterBytes);\n      start = cIn.getAdjustedStart();\n      end = cIn.getAdjustedEnd();\n      filePosition = cIn;\n    } else {\n      if (start != 0) {\n        // So we have a split that is only part of a file stored using\n        // a Compression codec that cannot be split.\n        throw new IOException(\"Cannot seek in \" +\n            codec.getClass().getSimpleName() + \" compressed stream\");\n      }\n\n      in = new SplitLineReader(codec.createInputStream(fileIn,\n          decompressor), job, this.recordDelimiterBytes);\n      filePosition = fileIn;\n    }\n  } else {\n    //如果文件是未压缩文件(普通文件)，则执行else中的语句\n    //跳转到文件中的指定位置\n    fileIn.seek(start);\n    //针对未压缩文件，创建一个阅读器读取一行一行的数据\n    in = new UncompressedSplitLineReader(\n        fileIn, job, this.recordDelimiterBytes, split.getLength());\n    filePosition = fileIn;\n  }\n  // If this is not the first split, we always throw away first record\n  // because we always (except the last split) read one extra line in\n  // next() method.\n  /*\n  注意：如果这个InputSplit不是第一个InputSplit，我们将会丢掉读取出来的第一行\n  因为我们总是通过next()方法多读取一行(会多读取下一个InputSplit的第一行)\n  这就解释了这个问题：如果一行数据被拆分到了两个InputSplit中，会不会有问题？\n  PPT中通过一个例子详细分析了这个问题\n   */\n  //如果start不等于0，表示不是第一个InputSplit，所以就把start的值重置为第二行的起始位置\n  if (start != 0) {\n    start += in.readLine(new Text(), 0, maxBytesToConsume(start));\n  }\n  this.pos = start;\n}\n\n分析完初始化方法，接下来看一下这个阅读器中最重要的方法nextKeyValue()\n/*\n这个方法是核心的方法，会被框架调用，每调用一次，就会读取一行数据，最终获取到我们之前说的\u003ck1,v1\u003e\n */\npublic boolean nextKeyValue() throws IOException {\n  if (key == null) {\n    key = new LongWritable();\n  }\n  // k1 就是每一行的起始位置\n  key.set(pos);\n  if (value == null) {\n    value = new Text();\n  }\n  int newSize = 0;\n  // We always read one extra line, which lies outside the upper\n  // split limit i.e. (end - 1)\n  while (getFilePosition() \u003c= end || in.needAdditionalRecordAfterSplit()) {\n    if (pos == 0) {\n      newSize = skipUtfByteOrderMark();\n    } else {\n      //读取一行数据，赋值给value，也就是v1\n      newSize = in.readLine(value, maxLineLength, maxBytesToConsume(pos));\n      pos += newSize;\n    }\n\n    if ((newSize == 0) || (newSize \u003c maxLineLength)) {\n      break;\n    }\n\n    // line too long. try again\n    LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + \n             (pos - newSize));\n  }\n  if (newSize == 0) {\n    key = null;\n    value = null;\n    return false;\n  } else {\n    return true;\n  }\n}\n\n","Properties":{"id":""}},{"Type":"NodeCodeBlockFenceCloseMarker","Data":"```","CodeBlockFenceLen":3,"Properties":{"id":""}}]}]}