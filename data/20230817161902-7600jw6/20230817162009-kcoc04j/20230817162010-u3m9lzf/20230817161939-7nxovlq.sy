{"ID":"20230817161939-7nxovlq","Spec":"1","Type":"NodeDocument","Properties":{"id":"20230817161939-7nxovlq","title":"MapReduce的执行流程","updated":"20230817161939"},"Children":[{"ID":"20230817161940-pegm02r","Type":"NodeHeading","HeadingLevel":4,"Properties":{"id":"20230817161940-pegm02r","updated":"20230817161940"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"#### ","Properties":{"id":""}},{"Type":"NodeText","Data":"MapReduce的执行流程"}]},{"ID":"20230817161941-bz6fm88","Type":"NodeParagraph","Properties":{"id":"20230817161941-bz6fm88","updated":"20230817161941"},"Children":[{"Type":"NodeImage","Properties":{"id":""},"Children":[{"Type":"NodeBang","Data":"!","Properties":{"id":""}},{"Type":"NodeOpenBracket","Data":"[","Properties":{"id":""}},{"Type":"NodeLinkText","Data":"map阶段","Properties":{"id":""}},{"Type":"NodeCloseBracket","Data":"]","Properties":{"id":""}},{"Type":"NodeOpenParen","Data":"(","Properties":{"id":""}},{"Type":"NodeLinkDest","Data":"C:\\Users\\HHH\\Desktop\\map阶段.jpg","Properties":{"id":""}},{"Type":"NodeCloseParen","Data":")","Properties":{"id":""}}]}]},{"ID":"20230817161942-bxxmput","Type":"NodeHTMLBlock","Data":"\u003cdiv\u003e\n\u003cimg src=\"C:\\Users\\HHH\\Desktop\\reduce阶段.png\" alt=\"reduce阶段\" style=\"zoom:75%;\" /\u003e\n\u003c/div\u003e","HtmlBlockType":7,"Properties":{"id":"20230817161942-bxxmput","updated":"20230817161942"}},{"ID":"20230817161943-zjau8wh","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20230817161943-zjau8wh","updated":"20230817161943"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"##### ","Properties":{"id":""}},{"Type":"NodeText","Data":"MapReduce之Map阶段"}]},{"ID":"20230817161944-8dwu0ks","Type":"NodeParagraph","Properties":{"id":"20230817161944-8dwu0ks","updated":"20230817161944"},"Children":[{"Type":"NodeText","Data":"mapreduce主要分为两大步骤 map和reduce，map和reduce在代码层面对应的就是两个类，map对应的是mapper类，reduce对应的是reducer类，下面我们就来根据一个案例具体分析一下这两个步骤"}]},{"ID":"20230817161945-xsof9gr","Type":"NodeParagraph","Properties":{"id":"20230817161945-xsof9gr","updated":"20230817161945"},"Children":[{"Type":"NodeText","Data":"假设我们有一个文件，文件里面有两行内容"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"第一行是hello you"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"第二行是hello me"}]},{"ID":"20230817161946-nrfi9b3","Type":"NodeParagraph","Properties":{"id":"20230817161946-nrfi9b3","updated":"20230817161946"},"Children":[{"Type":"NodeText","Data":"我们想统计文件中每个单词出现的总次数"}]},{"ID":"20230817161947-1r93alz","Type":"NodeParagraph","Properties":{"id":"20230817161947-1r93alz","updated":"20230817161947"},"Children":[{"Type":"NodeText","Data":"首先是map阶段"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"strong","TextMarkTextContent":"第一步"},{"Type":"NodeText","Data":"：框架会把输入文件(夹)划分为很多InputSplit，这里的inputsplit就是前面我们所说的split【对文件进行逻辑划分产生的】，默认情况下，每个HDFS的Block对应一个InputSplit。再通过RecordReader类，把每个InputSplit解析成一个一个的\u003ck1,v1\u003e。默认情况下，每一行数据，都会被解析成一个\u003ck1,v1\u003e"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"这里的k1是指每一行的起始偏移量，v1代表的是那一行内容，"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"所以，针对文件中的数据，经过map处理之后的结果是这样的"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003c0，hello you\u003e"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003c10，hello me\u003e"}]},{"ID":"20230817161948-foo7s5x","Type":"NodeParagraph","Properties":{"id":"20230817161948-foo7s5x","updated":"20230817161948"},"Children":[{"Type":"NodeText","Data":"注意：map第一次执行会产生\u003c0，hello you\u003e，第二次执行会产生\u003c10，hello me\u003e，并不是执行一次就获取到这两行结果了，因为每次只会读取一行数据，我在这里只是把这两行执行的最终结果都列出来了"}]},{"ID":"20230817161949-9oz5797","Type":"NodeParagraph","Properties":{"id":"20230817161949-9oz5797","updated":"20230817161949"},"Children":[{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"strong","TextMarkTextContent":"第二步"},{"Type":"NodeText","Data":"：框架调用Mapper类中的map(…)函数，map函数的输入是\u003ck1,v1\u003e，输出是\u003ck2,v2\u003e。一个InputSplit对应一个map task。程序员需要自己覆盖Mapper类中的map函数，实现具体的业务逻辑。"}]},{"ID":"20230817161950-62ditve","Type":"NodeParagraph","Properties":{"id":"20230817161950-62ditve","updated":"20230817161950"},"Children":[{"Type":"NodeText","Data":"因为我们需要统计文件中每个单词出现的总次数，所以需要先把每一行内容中的单词切开，然后记录出现次数为1,这个逻辑就需要我们在map函数中实现了"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"那针对\u003c0，hello you\u003e执行这个逻辑之后的结果就是"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003chello,1\u003e"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003cyou,1\u003e"}]},{"ID":"20230817161951-b2pqh8m","Type":"NodeParagraph","Properties":{"id":"20230817161951-b2pqh8m","updated":"20230817161951"},"Children":[{"Type":"NodeText","Data":"针对\u003c10，hello me\u003e执行这个逻辑之后的结果是"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003chello,1\u003e"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003cme,1\u003e"}]},{"ID":"20230817161952-cwqt8gb","Type":"NodeParagraph","Properties":{"id":"20230817161952-cwqt8gb","updated":"20230817161952"},"Children":[{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"strong","TextMarkTextContent":"第三步"},{"Type":"NodeText","Data":"：框架对map函数输出的\u003ck2,v2\u003e进行分区。不同分区中的\u003ck2,v2\u003e由不同的reduce task处理，默认只有1个分区，所以所有的数据都在一个分区，最后只会产生一个reduce task。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"经过这个步骤之后，数据没什么变化，如果有多个分区的话，需要把这些数据根据分区规则分开，在这里默认只有1个分区。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003chello,1\u003e"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003cyou,1\u003e"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003chello,1\u003e"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003cme,1\u003e"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"咱们在这所说的单词计数，其实就是把每个单词出现的次数进行汇总即可，需要进行全局的汇总，不需要进行分区，所以一个redeuce任务就可以搞定，"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"如果你的业务逻辑比较复杂，需要进行分区，那么就会产生多个reduce任务了，"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"那么这个时候，map任务输出的数据到底给哪个reduce使用？这个就需要划分一下，要不然就乱套了。假设有两个reduce，map的输出到底给哪个reduce，如何分配，这是一个问题。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"这个问题，由分区来完成。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"map输出的那些数据到底给哪个reduce使用，这个就是分区干的事了。"}]},{"ID":"20230817161953-vyd59y8","Type":"NodeParagraph","Properties":{"id":"20230817161953-vyd59y8","updated":"20230817161953"},"Children":[{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"strong","TextMarkTextContent":"第四步"},{"Type":"NodeText","Data":"：框架对每个分区中的数据，都会按照k2进行排序、分组。分组指的是相同k2的v2分成一个组。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"先按照k2排序"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003chello,1\u003e"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003chello,1\u003e"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003cme,1\u003e"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003cyou,1\u003e"}]},{"ID":"20230817161954-z00nqjw","Type":"NodeParagraph","Properties":{"id":"20230817161954-z00nqjw","updated":"20230817161954"},"Children":[{"Type":"NodeText","Data":"然后按照k2进行分组，把相同k2的v2分成一个组"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003chello,{1,1}\u003e"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003cme,{1}\u003e"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003cyou,{1}\u003e"}]},{"ID":"20230817161955-vx418zm","Type":"NodeParagraph","Properties":{"id":"20230817161955-vx418zm","updated":"20230817161955"},"Children":[{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"strong","TextMarkTextContent":"第五步"},{"Type":"NodeText","Data":"：在map阶段，框架可以选择执行Combiner过程"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"Combiner可以翻译为规约，规约是什么意思呢？ 在刚才的例子中，咱们最终是要在reduce端计算单词出现的总次数的，所以其实是可以在map端提前执行reduce的计算逻辑，先对在map端对单词出现的次数进行局部求和操作，这样就可以减少map端到reduce端数据传输的大小，这就是规约的好处，当然了，并不是所有场景都可以使用规约，针对求平均值之类的操作就不能使用规约了，否则最终计算的结果就不准确了。"}]},{"ID":"20230817161956-b271sco","Type":"NodeParagraph","Properties":{"id":"20230817161956-b271sco","updated":"20230817161956"},"Children":[{"Type":"NodeText","Data":"Combiner一个可选步骤，默认这个步骤是不执行的。"}]},{"ID":"20230817161957-dmewemb","Type":"NodeParagraph","Properties":{"id":"20230817161957-dmewemb","updated":"20230817161957"},"Children":[{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"strong","TextMarkTextContent":"第六步"},{"Type":"NodeText","Data":"：框架会把map task输出的\u003ck2,v2\u003e写入到linux 的磁盘文件中"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003chello,{1,1}\u003e"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003cme,{1}\u003e"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003cyou,{1}\u003e"}]},{"ID":"20230817161958-i12lwcn","Type":"NodeParagraph","Properties":{"id":"20230817161958-i12lwcn","updated":"20230817161958"},"Children":[{"Type":"NodeText","Data":"至此，整个map阶段执行结束"}]},{"ID":"20230817161959-mpvusub","Type":"NodeParagraph","Properties":{"id":"20230817161959-mpvusub","updated":"20230817161959"},"Children":[{"Type":"NodeText","Data":"最后注意一点："},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"MapReduce程序是由map和reduce这两个阶段组成的，但是reduce阶段不是必须的，也就是说有的mapreduce任务只有map阶段，为什么会有这种任务呢？"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"是这样的，咱们前面说过，其实reduce主要是做最终聚合的，如果我们这个需求是不需要聚合操作，直接对数据做过滤处理就行了，那也就意味着数据经过map阶段处理完就结束了，所以如果reduce阶段不存在的话，map的结果是可以直接保存到HDFS中的"}]},{"ID":"20230817161960-72ucwem","Type":"NodeParagraph","Properties":{"id":"20230817161960-72ucwem","updated":"20230817161960"},"Children":[{"Type":"NodeText","Data":"注意，如果没有reduce阶段，其实map阶段只需要执行到第二步就可以，第二步执行完成以后，结果就可以直接输出到HDFS了。"}]},{"ID":"20230817161961-u74nqjc","Type":"NodeParagraph","Properties":{"id":"20230817161961-u74nqjc","updated":"20230817161961"},"Children":[{"Type":"NodeText","Data":"针对我们这个单词计数的需求是存在reduce阶段的，所以我们继续往下面分析。"}]},{"ID":"20230817161962-ku97jsz","Type":"NodeHeading","HeadingLevel":5,"Properties":{"id":"20230817161962-ku97jsz","updated":"20230817161962"},"Children":[{"Type":"NodeHeadingC8hMarker","Data":"##### ","Properties":{"id":""}},{"Type":"NodeText","Data":"MapReduce之Reduce阶段"}]},{"ID":"20230817161963-369ykcy","Type":"NodeParagraph","Properties":{"id":"20230817161963-369ykcy","updated":"20230817161963"},"Children":[{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"strong","TextMarkTextContent":"第一步"},{"Type":"NodeText","Data":"：框架对多个map任务的输出，按照不同的分区，通过网络copy到不同的reduce节点。这个过程称作shuffle"}]},{"ID":"20230817161964-oufntj8","Type":"NodeParagraph","Properties":{"id":"20230817161964-oufntj8","updated":"20230817161964"},"Children":[{"Type":"NodeText","Data":"针对我们这个需求，只有一个分区，所以把数据拷贝到reduce端之后还是老样子"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003chello,{1,1}\u003e"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003cme,{1}\u003e"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003cyou,{1}\u003e"}]},{"ID":"20230817161965-c2mmdce","Type":"NodeParagraph","Properties":{"id":"20230817161965-c2mmdce","updated":"20230817161965"},"Children":[{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"strong","TextMarkTextContent":"第二步"},{"Type":"NodeText","Data":"：框架对reduce端接收的相同分区的\u003ck2,v2\u003e数据进行合并、排序、分组。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"reduce端接收到的是多个map的输出，对多个map任务中相同分区的数据进行合并 排序 分组"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"注意，之前在map中已经做了排序 分组，这边也做这些操作 重复吗？"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"不重复，因为map端是局部的操作 reduce端是全局的操作"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"之前是每个map任务内进行排序，是有序的，但是多个map任务之间就是无序的了。"}]},{"ID":"20230817161966-g2ey35t","Type":"NodeParagraph","Properties":{"id":"20230817161966-g2ey35t","updated":"20230817161966"},"Children":[{"Type":"NodeText","Data":"不过针对我们这个需求只有一个map任务一个分区，所以最终的结果还是老样子"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003chello,{1,1}\u003e"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003cme,{1}\u003e"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003cyou,{1}\u003e"}]},{"ID":"20230817161967-rrslm2w","Type":"NodeParagraph","Properties":{"id":"20230817161967-rrslm2w","updated":"20230817161967"},"Children":[{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"strong","TextMarkTextContent":"第三步"},{"Type":"NodeText","Data":"：框架调用Reducer类中的reduce方法，reduce方法的输入是\u003ck2,{v2}\u003e，输出是\u003ck3,v3\u003e。一个\u003ck2,{v2}\u003e调用一次reduce函数。程序员需要覆盖reduce函数，实现具体的业务逻辑。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"那我们在这里就需要在reduce函数中实现最终的聚合计算操作了，将相同k2的{v2}累加求和，然后再转化为k3,v3写出去，在这里最终会调用三次reduce函数"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003chello,2\u003e"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003cme,1\u003e"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"\u003cyou,1\u003e"}]},{"ID":"20230817161968-oieun2f","Type":"NodeParagraph","Properties":{"id":"20230817161968-oieun2f","updated":"20230817161968"},"Children":[{"Type":"NodeTextMark","Properties":{"id":""},"TextMarkType":"strong","TextMarkTextContent":"第四步"},{"Type":"NodeText","Data":"：框架把reduce的输出结果保存到HDFS中。"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"hello 2"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"me 1"},{"Type":"NodeSoftBreak","Data":"\n","Properties":{"id":""}},{"Type":"NodeText","Data":"you 1"}]},{"ID":"20230817161969-fnky16z","Type":"NodeParagraph","Properties":{"id":"20230817161969-fnky16z","updated":"20230817161969"},"Children":[{"Type":"NodeText","Data":"至此，整个reduce阶段结束。"}]}]}